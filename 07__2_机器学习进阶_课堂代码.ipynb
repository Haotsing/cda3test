{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd23e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81960a",
   "metadata": {},
   "source": [
    "# 需求\n",
    "使用大部分标签缺失的数据集'保险数据_半监督学习.csv'，训练一个分类器，实现类别标签的预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a744ba7c",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416d74da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 数据预处理计算函数\n",
    "def preprocessing(df):\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    \n",
    "    cat_cols= df.select_dtypes(include=[\"object\"])   # 分类型变量\n",
    "    num_cols= df.select_dtypes(include=[\"int\", \"float\"])   # 数值型变量\n",
    "\n",
    "    # 连续型数据\n",
    "    num_imp= SimpleImputer(strategy='mean')  # 缺失值\n",
    "    num_std= StandardScaler()  # 标准化\n",
    "    num_pipeline= Pipeline(steps=[(\"num_imp\", num_imp), (\"num_std\", num_std)])\n",
    "    # 分类型数据\n",
    "    cat_imp= SimpleImputer(strategy=\"most_frequent\")  # 缺失值\n",
    "    cat_encode= OrdinalEncoder()   # 数据编码\n",
    "    cat_pipeline= Pipeline(steps=[(\"cat_imp\", cat_imp), (\"cat_encode\", cat_encode)])\n",
    "\n",
    "    col_trans= ColumnTransformer(transformers=[(\"num_pipeline\", num_pipeline, num_cols.columns),\n",
    "                                           (\"cat_pipeline\", cat_pipeline, cat_cols.columns),])\n",
    "    # 数据集处理的计算\n",
    "    transfer= col_trans.fit(df)\n",
    "    return transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8d4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "raw_data= pd.read_csv('./data/保险数据_半监督学习.csv')\n",
    "labels= raw_data.pop(\"resp_flag\")  # y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b17734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缺失值/总样本：30000/39173\n"
     ]
    }
   ],
   "source": [
    "# 只有少部分样本有标签\n",
    "print(\"缺失值/总样本：\"+str(labels.isnull().sum())+\"/\"+str(len(labels)))\n",
    "# print(\"类别比例：\"+str(labels.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e39035",
   "metadata": {},
   "source": [
    "sklearn中的半监督学习算法要求，所有缺失的标签必须用-1填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10b0a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用-1填充标签\n",
    "labels_fill= labels.fillna(-1)\n",
    "\n",
    "# 特征数据处理\n",
    "transfer= preprocessing(raw_data)\n",
    "data_trans= transfer.transform(raw_data)\n",
    "\n",
    "data_concat= pd.concat([labels_fill, pd.DataFrame(data_trans)], axis= 1)\n",
    "\n",
    "# 保存一部分有标签样本作为测试集\n",
    "mask_labeled= (labels_fill != -1)\n",
    "mask_unlabeled= (labels_fill == -1)\n",
    "\n",
    "data_labeled= data_concat[mask_labeled]\n",
    "data_unlabeled= data_concat[mask_unlabeled]\n",
    "\n",
    "# 切分测试集\n",
    "from sklearn.model_selection  import train_test_split\n",
    "train, test= train_test_split(data_labeled, test_size=0.2, stratify= data_labeled[\"resp_flag\"], random_state= 42)\n",
    "\n",
    "Xtrain= pd.concat([train, data_unlabeled])\n",
    "Ytrain= Xtrain.pop(\"resp_flag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbeb86c",
   "metadata": {},
   "source": [
    "# Semi supervised learning（半监督学习）\n",
    "## Self Training\n",
    "自训练（Self-training）:\n",
    "\n",
    "自训练是一种简单的半监督学习方法，它首先使用已标记的数据训练一个监督学习模型。然后，该模型用于预测未标记数据的标签。预测最自信的标签可以被选择添加到训练集中，然后模型在新的、更大的训练集上重新训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55f5a12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SelfTrainingClassifier(base_estimator=RandomForestClassifier(oob_score=True))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelfTrainingClassifier</label><div class=\"sk-toggleable__content\"><pre>SelfTrainingClassifier(base_estimator=RandomForestClassifier(oob_score=True))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(oob_score=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(oob_score=True)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "SelfTrainingClassifier(base_estimator=RandomForestClassifier(oob_score=True))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8883257485029942\n"
     ]
    }
   ],
   "source": [
    "# Base Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF= RandomForestClassifier(oob_score=True)\n",
    "\n",
    "# Self Training\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "RF_self_training= SelfTrainingClassifier(RF)\n",
    "RF_self_training.fit(Xtrain, Ytrain)\n",
    "\n",
    "# 测试集模型评估\n",
    "Xtest= test\n",
    "Ytest= Xtest.pop(\"resp_flag\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"AUC: \", roc_auc_score(Ytest, RF_self_training.predict_proba(Xtest)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0a574",
   "metadata": {},
   "source": [
    "## Label Propagation \n",
    "### Label Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d76a09fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09, 0.15, 0.05, ..., 0.02, 0.96, 0.92])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_self_training.predict_proba(Xtest)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a1c4813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelPropagation(kernel=&#x27;knn&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelPropagation</label><div class=\"sk-toggleable__content\"><pre>LabelPropagation(kernel=&#x27;knn&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelPropagation(kernel='knn')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Propagation\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "\n",
    "label_propagation = LabelPropagation(kernel=\"knn\")\n",
    "label_propagation.fit(Xtrain, Ytrain)\n",
    "\n",
    "Ytrain_propagation= label_propagation.transduction_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0173048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11be5333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(oob_score=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(oob_score=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(oob_score=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8719227544910179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF_propagation= RandomForestClassifier(oob_score=True)\n",
    "RF_propagation.fit(Xtrain, Ytrain_propagation)\n",
    "\n",
    "print(\"AUC: \", roc_auc_score(Ytest, RF_propagation.predict_proba(Xtest)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a8fe2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7 , 0.3 ],\n",
       "       [0.65, 0.35],\n",
       "       [0.96, 0.04],\n",
       "       ...,\n",
       "       [0.99, 0.01],\n",
       "       [0.28, 0.72],\n",
       "       [0.61, 0.39]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_propagation.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f056b7b4",
   "metadata": {},
   "source": [
    "### Label Spreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfa4dc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelSpreading(kernel=&#x27;knn&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelSpreading</label><div class=\"sk-toggleable__content\"><pre>LabelSpreading(kernel=&#x27;knn&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelSpreading(kernel='knn')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Spreading\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "\n",
    "label_spreading = LabelSpreading(kernel=\"knn\", alpha= 0.2)\n",
    "label_spreading.fit(Xtrain, Ytrain)\n",
    "\n",
    "Ytrain_spreading= label_spreading.transduction_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54655d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 0., 1., 1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain_spreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa0be023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(oob_score=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(oob_score=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(oob_score=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8787538922155689\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF_spreading= RandomForestClassifier(oob_score=True)\n",
    "RF_spreading.fit(Xtrain, Ytrain_spreading)\n",
    "\n",
    "print(\"AUC: \", roc_auc_score(Ytest, RF_spreading.predict_proba(Xtest)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa10c0",
   "metadata": {},
   "source": [
    "选定Label Spreading方法，尝试更多模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6772048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8914311377245508\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "XGB = XGBClassifier()\n",
    "XGB.fit(Xtrain, Ytrain_spreading)\n",
    "\n",
    "print(\"AUC: \", roc_auc_score(Ytest, XGB.predict_proba(Xtest)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c48c5355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "AUC:  0.8994946107784432\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "import lightgbm\n",
    "\n",
    "train= lightgbm.Dataset(Xtrain, label= Ytrain_spreading)\n",
    "test= lightgbm.Dataset(Xtest, label= Ytest)\n",
    "\n",
    "lgb = lightgbm.train(params={}, train_set= train, valid_sets= test)\n",
    "\n",
    "print(\"AUC: \", roc_auc_score(Ytest, lgb.predict(Xtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d7c995",
   "metadata": {},
   "source": [
    "# Tuning the hyper-parameters（调参）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f44eb4",
   "metadata": {},
   "source": [
    "## Grid Search（网格搜索）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dff4828e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     grow_policy=None, importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.8, 0.9],\n",
       "                         &#x27;max_depth&#x27;: array([3, 4, 5]),\n",
       "                         &#x27;n_estimators&#x27;: array([100, 110, 120, 130, 140]),\n",
       "                         &#x27;subsample&#x27;: [0.8, 0.9]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     grow_policy=None, importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.8, 0.9],\n",
       "                         &#x27;max_depth&#x27;: array([3, 4, 5]),\n",
       "                         &#x27;n_estimators&#x27;: array([100, 110, 120, 130, 140]),\n",
       "                         &#x27;subsample&#x27;: [0.8, 0.9]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     grow_policy=None, importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.8, 0.9],\n",
       "                         'max_depth': array([3, 4, 5]),\n",
       "                         'n_estimators': array([100, 110, 120, 130, 140]),\n",
       "                         'subsample': [0.8, 0.9]})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8931916167664672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.9,\n",
       " 'max_depth': 5,\n",
       " 'n_estimators': 120,\n",
       " 'subsample': 0.9}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param = {\"n_estimators\": np.arange(100,150,20),    # 启，止，步长\n",
    "         \"max_depth\": np.arange(3,6,2),\n",
    "        \"subsample\": [0.8, 0.9],\n",
    "        \"colsample_bytree\": [0.8, 0.9]}\n",
    "\n",
    "XGB = XGBClassifier()\n",
    "grid_XGB = GridSearchCV(XGB, param, cv=3, n_jobs=-1)  \n",
    "grid_XGB.fit(Xtrain, Ytrain_spreading)\n",
    "\n",
    "print(\"AUC: \", roc_auc_score(Ytest, grid_XGB.predict_proba(Xtest)[:, 1]))\n",
    "grid_XGB.best_params_ #最优参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e57c3",
   "metadata": {},
   "source": [
    "## Halving Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56d0ecb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 8\n",
      "n_required_iterations: 8\n",
      "n_possible_iterations: 8\n",
      "min_resources_: 17\n",
      "max_resources_: 37338\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 3750\n",
      "n_resources: 17\n",
      "Fitting 3 folds for each of 3750 candidates, totalling 11250 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 1250\n",
      "n_resources: 51\n",
      "Fitting 3 folds for each of 1250 candidates, totalling 3750 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 417\n",
      "n_resources: 153\n",
      "Fitting 3 folds for each of 417 candidates, totalling 1251 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 139\n",
      "n_resources: 459\n",
      "Fitting 3 folds for each of 139 candidates, totalling 417 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 47\n",
      "n_resources: 1377\n",
      "Fitting 3 folds for each of 47 candidates, totalling 141 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 16\n",
      "n_resources: 4131\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 6\n",
      "n_resources: 12393\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 2\n",
      "n_resources: 37179\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(cv=3,\n",
       "                    estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=None,\n",
       "                                            feature_types=None, gamma=None,\n",
       "                                            grow_policy=None,\n",
       "                                            importance_type=None,\n",
       "                                            interaction_constraints=None,\n",
       "                                            learning_rat...\n",
       "                    param_grid={&#x27;colsample_bytree&#x27;: array([0.5, 0.6, 0.7, 0.8, 0.9]),\n",
       "                                &#x27;max_depth&#x27;: array([3, 4, 5]),\n",
       "                                &#x27;n_estimators&#x27;: array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "       113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
       "       139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]),\n",
       "                                &#x27;subsample&#x27;: array([0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "                    verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(cv=3,\n",
       "                    estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=None,\n",
       "                                            feature_types=None, gamma=None,\n",
       "                                            grow_policy=None,\n",
       "                                            importance_type=None,\n",
       "                                            interaction_constraints=None,\n",
       "                                            learning_rat...\n",
       "                    param_grid={&#x27;colsample_bytree&#x27;: array([0.5, 0.6, 0.7, 0.8, 0.9]),\n",
       "                                &#x27;max_depth&#x27;: array([3, 4, 5]),\n",
       "                                &#x27;n_estimators&#x27;: array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "       113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
       "       139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]),\n",
       "                                &#x27;subsample&#x27;: array([0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "                    verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(cv=3,\n",
       "                    estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=None,\n",
       "                                            feature_types=None, gamma=None,\n",
       "                                            grow_policy=None,\n",
       "                                            importance_type=None,\n",
       "                                            interaction_constraints=None,\n",
       "                                            learning_rat...\n",
       "                    param_grid={'colsample_bytree': array([0.5, 0.6, 0.7, 0.8, 0.9]),\n",
       "                                'max_depth': array([3, 4, 5]),\n",
       "                                'n_estimators': array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "       113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
       "       139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]),\n",
       "                                'subsample': array([0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "                    verbose=1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9001964071856288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.7999999999999999,\n",
       " 'max_depth': 3,\n",
       " 'n_estimators': 108,\n",
       " 'subsample': 0.7}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Halving 搜索\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "param = {\"n_estimators\": np.arange(100,150,1),    # 启，止，步长\n",
    "         \"max_depth\": np.arange(3,6,1),\n",
    "        \"subsample\": np.arange(0.5,1,0.1),\n",
    "        \"colsample_bytree\": np.arange(0.5,1,0.1)}\n",
    "\n",
    "XGB = XGBClassifier()\n",
    "Hal_XGB = HalvingGridSearchCV(XGB, param, factor=3, cv=3, verbose=1, n_jobs=-1)  \n",
    "Hal_XGB.fit(Xtrain, Ytrain_spreading)\n",
    "\n",
    "print(\"AUC: \", roc_auc_score(Ytest, Hal_XGB.predict_proba(Xtest)[:, 1]))\n",
    "Hal_XGB.best_params_ #最优参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab30621",
   "metadata": {},
   "source": [
    "## Randomized Search（随机搜索）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ac761e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, device=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=None, feature_types=None,\n",
       "                                           gamma=None, grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate...\n",
       "                                        &#x27;max_depth&#x27;: array([3, 4, 5]),\n",
       "                                        &#x27;n_estimators&#x27;: array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "       113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
       "       139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]),\n",
       "                                        &#x27;subsample&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000269924D1B50&gt;},\n",
       "                   verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, device=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=None, feature_types=None,\n",
       "                                           gamma=None, grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate...\n",
       "                                        &#x27;max_depth&#x27;: array([3, 4, 5]),\n",
       "                                        &#x27;n_estimators&#x27;: array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "       113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
       "       139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]),\n",
       "                                        &#x27;subsample&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000269924D1B50&gt;},\n",
       "                   verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, device=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=None, feature_types=None,\n",
       "                                           gamma=None, grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate...\n",
       "                                        'max_depth': array([3, 4, 5]),\n",
       "                                        'n_estimators': array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "       113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
       "       139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]),\n",
       "                                        'subsample': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000269924D1B50>},\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8946634730538923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.6502885285360598,\n",
       " 'max_depth': 5,\n",
       " 'n_estimators': 141,\n",
       " 'subsample': 0.9380730375335327}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随机搜索\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "param = dict(n_estimators= np.arange(100,150,1),    # 启，止，步长\n",
    "         max_depth= np.arange(3,6,1),\n",
    "         subsample= uniform(loc=0.5, scale=0.5),   # 启，增加量\n",
    "         colsample_bytree= uniform(loc=0.5, scale=0.5))\n",
    "\n",
    "XGB = XGBClassifier()\n",
    "Rand_XGB = RandomizedSearchCV(XGB, param, cv=3, n_iter=50, verbose=1, n_jobs=-1)  \n",
    "Rand_XGB.fit(Xtrain, Ytrain_spreading)\n",
    "\n",
    "print(\"AUC: \", roc_auc_score(Ytest, Rand_XGB.predict_proba(Xtest)[:, 1]))\n",
    "Rand_XGB.best_params_ #最优参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd6c255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看最优模型\n",
    "best_model = Rand_XGB.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f8b40",
   "metadata": {},
   "source": [
    "## Bayesian Optimization（贝叶斯搜索）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1d47b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:13:53,876] A new study created in memory with name: no-name-6db991b9-af27-4c12-8191-7670c3c6996b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:13:54,420] Trial 0 finished with value: 0.8976574850299401 and parameters: {'lambda_l2': 1.3232105324442147, 'num_leaves': 109, 'feature_fraction': 0.5073613631447758, 'bagging_fraction': 0.5761853706701012}. Best is trial 0 with value: 0.8976574850299401.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:13:54,827] Trial 1 finished with value: 0.8986359281437125 and parameters: {'lambda_l2': 5.014173649656192, 'num_leaves': 91, 'feature_fraction': 0.8856409218111932, 'bagging_fraction': 0.4820728208916143}. Best is trial 1 with value: 0.8986359281437125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:13:55,393] Trial 2 finished with value: 0.8985077844311378 and parameters: {'lambda_l2': 5.722135066167465, 'num_leaves': 141, 'feature_fraction': 0.7007096167441835, 'bagging_fraction': 0.590507211570766}. Best is trial 1 with value: 0.8986359281437125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:13:55,895] Trial 3 finished with value: 0.899762874251497 and parameters: {'lambda_l2': 4.837582857267608, 'num_leaves': 84, 'feature_fraction': 0.600511420532717, 'bagging_fraction': 0.9077499064567501}. Best is trial 3 with value: 0.899762874251497.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:13:56,306] Trial 4 finished with value: 0.9016778443113772 and parameters: {'lambda_l2': 8.383911335389806, 'num_leaves': 92, 'feature_fraction': 0.794188317545981, 'bagging_fraction': 0.5635563004599092}. Best is trial 4 with value: 0.9016778443113772.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:13:57,101] Trial 5 finished with value: 0.8952215568862276 and parameters: {'lambda_l2': 0.3992909985416842, 'num_leaves': 158, 'feature_fraction': 0.6654149424885374, 'bagging_fraction': 0.7128934326691614}. Best is trial 4 with value: 0.9016778443113772.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:13:57,467] Trial 6 finished with value: 0.9009916167664671 and parameters: {'lambda_l2': 9.481266145084406, 'num_leaves': 72, 'feature_fraction': 0.6850083336537507, 'bagging_fraction': 0.6314883931532087}. Best is trial 4 with value: 0.9016778443113772.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:13:57,794] Trial 7 finished with value: 0.9025688622754491 and parameters: {'lambda_l2': 9.495352901832101, 'num_leaves': 65, 'feature_fraction': 0.8928810656514024, 'bagging_fraction': 0.8003060921157638}. Best is trial 7 with value: 0.9025688622754491.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:13:58,808] Trial 8 finished with value: 0.8935437125748502 and parameters: {'lambda_l2': 8.138560736208225, 'num_leaves': 248, 'feature_fraction': 0.5183388154145385, 'bagging_fraction': 0.6019354623944839}. Best is trial 7 with value: 0.9025688622754491.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:13:59,661] Trial 9 finished with value: 0.8931688622754491 and parameters: {'lambda_l2': 9.308871563847932, 'num_leaves': 191, 'feature_fraction': 0.7062810426182772, 'bagging_fraction': 0.8177122312509599}. Best is trial 7 with value: 0.9025688622754491.\n",
      "[I 2024-05-19 17:13:59,820] Trial 10 finished with value: 0.8936520958083833 and parameters: {'lambda_l2': 6.722446175233479, 'num_leaves': 6, 'feature_fraction': 0.9927716193933593, 'bagging_fraction': 0.9709670984762506}. Best is trial 7 with value: 0.9025688622754491.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:00,051] Trial 11 finished with value: 0.9048407185628742 and parameters: {'lambda_l2': 7.506018430605858, 'num_leaves': 29, 'feature_fraction': 0.8308817376836449, 'bagging_fraction': 0.7841311854491992}. Best is trial 11 with value: 0.9048407185628742.\n",
      "[I 2024-05-19 17:14:00,227] Trial 12 finished with value: 0.9027329341317366 and parameters: {'lambda_l2': 7.2001771623407835, 'num_leaves': 15, 'feature_fraction': 0.8721812571030565, 'bagging_fraction': 0.7806978297223499}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:00,348] Trial 13 finished with value: 0.8930497005988023 and parameters: {'lambda_l2': 3.0500598363724185, 'num_leaves': 5, 'feature_fraction': 0.8393421079369927, 'bagging_fraction': 0.7641134985033863}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:00,717] Trial 14 finished with value: 0.8987868263473053 and parameters: {'lambda_l2': 6.967548493603552, 'num_leaves': 33, 'feature_fraction': 0.9932918364136395, 'bagging_fraction': 0.8440455737735086}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007823 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:01,013] Trial 15 finished with value: 0.9024814371257485 and parameters: {'lambda_l2': 7.157440410242269, 'num_leaves': 42, 'feature_fraction': 0.7962846547958189, 'bagging_fraction': 0.7088502298695816}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:01,271] Trial 16 finished with value: 0.903443113772455 and parameters: {'lambda_l2': 3.548301353706993, 'num_leaves': 35, 'feature_fraction': 0.9257414822910617, 'bagging_fraction': 0.8946403681514781}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:01,561] Trial 17 finished with value: 0.9016574850299401 and parameters: {'lambda_l2': 3.2739845355363775, 'num_leaves': 47, 'feature_fraction': 0.9273126895035209, 'bagging_fraction': 0.9937625329312594}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:02,764] Trial 18 finished with value: 0.8951676646706587 and parameters: {'lambda_l2': 3.488982895543828, 'num_leaves': 186, 'feature_fraction': 0.7848096863281677, 'bagging_fraction': 0.8988382580716474}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:03,592] Trial 19 finished with value: 0.8964634730538922 and parameters: {'lambda_l2': 1.8017130403470007, 'num_leaves': 128, 'feature_fraction': 0.9461140355591972, 'bagging_fraction': 0.9145782303855176}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:03,972] Trial 20 finished with value: 0.8987365269461077 and parameters: {'lambda_l2': 4.2885048876037875, 'num_leaves': 53, 'feature_fraction': 0.43474863833052857, 'bagging_fraction': 0.4108603869946413}. Best is trial 11 with value: 0.9048407185628742.\n",
      "[I 2024-05-19 17:14:04,193] Trial 21 finished with value: 0.9035700598802394 and parameters: {'lambda_l2': 6.132605159781168, 'num_leaves': 22, 'feature_fraction': 0.8488129729434505, 'bagging_fraction': 0.766539774081932}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:04,457] Trial 22 finished with value: 0.9041892215568861 and parameters: {'lambda_l2': 5.965920727237883, 'num_leaves': 28, 'feature_fraction': 0.8321421851748412, 'bagging_fraction': 0.8620424756734284}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:04,700] Trial 23 finished with value: 0.9033892215568862 and parameters: {'lambda_l2': 5.732074310643015, 'num_leaves': 23, 'feature_fraction': 0.7464833964984637, 'bagging_fraction': 0.662677186910335}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:05,020] Trial 24 finished with value: 0.8971832335329342 and parameters: {'lambda_l2': 6.118562450631044, 'num_leaves': 58, 'feature_fraction': 0.8359566767501159, 'bagging_fraction': 0.7518951109470787}. Best is trial 11 with value: 0.9048407185628742.\n",
      "[I 2024-05-19 17:14:05,233] Trial 25 finished with value: 0.9033760479041917 and parameters: {'lambda_l2': 8.131613887676595, 'num_leaves': 25, 'feature_fraction': 0.8322623265333854, 'bagging_fraction': 0.8730701755501752}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:05,333] Trial 26 finished with value: 0.8827604790419161 and parameters: {'lambda_l2': 6.520621257065276, 'num_leaves': 3, 'feature_fraction': 0.7525923607021983, 'bagging_fraction': 0.8416637606517392}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:05,917] Trial 27 finished with value: 0.8983413173652695 and parameters: {'lambda_l2': 7.890222139260423, 'num_leaves': 106, 'feature_fraction': 0.6273318074600586, 'bagging_fraction': 0.7413030883470244}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:06,282] Trial 28 finished with value: 0.9003508982035927 and parameters: {'lambda_l2': 5.018147677276181, 'num_leaves': 71, 'feature_fraction': 0.7401693783139975, 'bagging_fraction': 0.6967161491750494}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:06,803] Trial 29 finished with value: 0.898837125748503 and parameters: {'lambda_l2': 8.757743167047135, 'num_leaves': 115, 'feature_fraction': 0.8670907551157101, 'bagging_fraction': 0.9435603784402945}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:07,788] Trial 30 finished with value: 0.8933185628742515 and parameters: {'lambda_l2': 7.566255015505581, 'num_leaves': 232, 'feature_fraction': 0.9475150891908984, 'bagging_fraction': 0.8468436177022456}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:08,208] Trial 31 finished with value: 0.9032634730538922 and parameters: {'lambda_l2': 4.232316386333411, 'num_leaves': 35, 'feature_fraction': 0.9117276048906929, 'bagging_fraction': 0.819632591017448}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008712 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:08,505] Trial 32 finished with value: 0.9021999999999999 and parameters: {'lambda_l2': 2.485434773952413, 'num_leaves': 24, 'feature_fraction': 0.8359395686961976, 'bagging_fraction': 0.8710706942883735}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:08,888] Trial 33 finished with value: 0.9032023952095808 and parameters: {'lambda_l2': 5.8173576050465385, 'num_leaves': 54, 'feature_fraction': 0.9544084903567333, 'bagging_fraction': 0.9358014980625932}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:09,326] Trial 34 finished with value: 0.8994814371257485 and parameters: {'lambda_l2': 5.259861027971672, 'num_leaves': 83, 'feature_fraction': 0.8957955777136666, 'bagging_fraction': 0.7867753053744551}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:09,682] Trial 35 finished with value: 0.9016994011976047 and parameters: {'lambda_l2': 4.295442250706624, 'num_leaves': 42, 'feature_fraction': 0.8015285623352868, 'bagging_fraction': 0.8655078369784333}. Best is trial 11 with value: 0.9048407185628742.\n",
      "[I 2024-05-19 17:14:09,876] Trial 36 finished with value: 0.9027005988023951 and parameters: {'lambda_l2': 6.3865905018911295, 'num_leaves': 17, 'feature_fraction': 0.8475833260819651, 'bagging_fraction': 0.5071159303993423}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:10,397] Trial 37 finished with value: 0.9006694610778443 and parameters: {'lambda_l2': 5.375985577779825, 'num_leaves': 83, 'feature_fraction': 0.77231122479861, 'bagging_fraction': 0.7282044326939628}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:10,995] Trial 38 finished with value: 0.8971892215568862 and parameters: {'lambda_l2': 0.06368386836262463, 'num_leaves': 102, 'feature_fraction': 0.9690914831862008, 'bagging_fraction': 0.6856115947982686}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:11,710] Trial 39 finished with value: 0.8952766467065868 and parameters: {'lambda_l2': 3.7787467339878575, 'num_leaves': 153, 'feature_fraction': 0.9166928306507148, 'bagging_fraction': 0.8970664214541579}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:12,118] Trial 40 finished with value: 0.9016335329341316 and parameters: {'lambda_l2': 8.705113264863506, 'num_leaves': 66, 'feature_fraction': 0.7209556778655148, 'bagging_fraction': 0.8083578276094769}. Best is trial 11 with value: 0.9048407185628742.\n",
      "[I 2024-05-19 17:14:12,300] Trial 41 finished with value: 0.9020754491017964 and parameters: {'lambda_l2': 5.845049815158698, 'num_leaves': 15, 'feature_fraction': 0.7503946680661676, 'bagging_fraction': 0.6716422630771799}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:12,652] Trial 42 finished with value: 0.9048335329341317 and parameters: {'lambda_l2': 5.515773690564095, 'num_leaves': 29, 'feature_fraction': 0.811393339894536, 'bagging_fraction': 0.6715052148962826}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:12,915] Trial 43 finished with value: 0.9044119760479044 and parameters: {'lambda_l2': 4.834483070523266, 'num_leaves': 34, 'feature_fraction': 0.8165308233629336, 'bagging_fraction': 0.6420310357505762}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:13,227] Trial 44 finished with value: 0.9037437125748502 and parameters: {'lambda_l2': 4.71091932703137, 'num_leaves': 30, 'feature_fraction': 0.810441059302799, 'bagging_fraction': 0.6427803492865033}. Best is trial 11 with value: 0.9048407185628742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:13,533] Trial 45 finished with value: 0.9057269461077845 and parameters: {'lambda_l2': 4.768530754185674, 'num_leaves': 34, 'feature_fraction': 0.8205156123700014, 'bagging_fraction': 0.6303534743529644}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:13,827] Trial 46 finished with value: 0.9003041916167667 and parameters: {'lambda_l2': 4.554083395090186, 'num_leaves': 45, 'feature_fraction': 0.6569451914976264, 'bagging_fraction': 0.6041887527342269}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:14,153] Trial 47 finished with value: 0.9016395209580839 and parameters: {'lambda_l2': 5.384654370825359, 'num_leaves': 60, 'feature_fraction': 0.872796532261193, 'bagging_fraction': 0.5554275735116291}. Best is trial 45 with value: 0.9057269461077845.\n",
      "[I 2024-05-19 17:14:14,254] Trial 48 finished with value: 0.8634898203592815 and parameters: {'lambda_l2': 7.5000155044928976, 'num_leaves': 2, 'feature_fraction': 0.6786861837888976, 'bagging_fraction': 0.6334353943572167}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001857 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:14,418] Trial 49 finished with value: 0.9026371257485031 and parameters: {'lambda_l2': 6.800485558955561, 'num_leaves': 13, 'feature_fraction': 0.8171459796628333, 'bagging_fraction': 0.5394791722221489}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:14,909] Trial 50 finished with value: 0.8997796407185629 and parameters: {'lambda_l2': 2.7392791003817973, 'num_leaves': 74, 'feature_fraction': 0.5385246754012549, 'bagging_fraction': 0.5929024904917829}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:15,146] Trial 51 finished with value: 0.9020826347305388 and parameters: {'lambda_l2': 4.641939834687152, 'num_leaves': 33, 'feature_fraction': 0.808863238285832, 'bagging_fraction': 0.6410580528695959}. Best is trial 45 with value: 0.9057269461077845.\n",
      "[I 2024-05-19 17:14:15,373] Trial 52 finished with value: 0.9019664670658682 and parameters: {'lambda_l2': 4.916172758099008, 'num_leaves': 30, 'feature_fraction': 0.7792968964252545, 'bagging_fraction': 0.6165731658300782}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:15,664] Trial 53 finished with value: 0.8989077844311378 and parameters: {'lambda_l2': 3.9654884119440665, 'num_leaves': 46, 'feature_fraction': 0.7181801177795322, 'bagging_fraction': 0.6529320517325174}. Best is trial 45 with value: 0.9057269461077845.\n",
      "[I 2024-05-19 17:14:15,873] Trial 54 finished with value: 0.8985101796407184 and parameters: {'lambda_l2': 9.952954487338873, 'num_leaves': 11, 'feature_fraction': 0.7715391943442403, 'bagging_fraction': 0.7106550953538978}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:16,196] Trial 55 finished with value: 0.9027628742514969 and parameters: {'lambda_l2': 5.348461157217519, 'num_leaves': 38, 'feature_fraction': 0.8657106843428988, 'bagging_fraction': 0.6767658032297822}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:16,507] Trial 56 finished with value: 0.902011976047904 and parameters: {'lambda_l2': 6.2391134243399655, 'num_leaves': 51, 'feature_fraction': 0.8943117515419681, 'bagging_fraction': 0.568706019809844}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:16,943] Trial 57 finished with value: 0.9031676646706586 and parameters: {'lambda_l2': 4.896138255658673, 'num_leaves': 93, 'feature_fraction': 0.8057320723577064, 'bagging_fraction': 0.6194832196775316}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:17,228] Trial 58 finished with value: 0.9025101796407187 and parameters: {'lambda_l2': 4.546151398565156, 'num_leaves': 26, 'feature_fraction': 0.8200465060550884, 'bagging_fraction': 0.7245793202587588}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:18,002] Trial 59 finished with value: 0.8960910179640719 and parameters: {'lambda_l2': 3.9059385287864674, 'num_leaves': 195, 'feature_fraction': 0.8649659510536055, 'bagging_fraction': 0.5832422570655326}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:18,360] Trial 60 finished with value: 0.9011437125748503 and parameters: {'lambda_l2': 7.095302242933419, 'num_leaves': 61, 'feature_fraction': 0.7609799147365215, 'bagging_fraction': 0.5339932867513653}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:18,617] Trial 61 finished with value: 0.9021928143712575 and parameters: {'lambda_l2': 6.09018519779916, 'num_leaves': 21, 'feature_fraction': 0.8531506100035959, 'bagging_fraction': 0.7616967422988208}. Best is trial 45 with value: 0.9057269461077845.\n",
      "[I 2024-05-19 17:14:18,767] Trial 62 finished with value: 0.8994479041916167 and parameters: {'lambda_l2': 5.60625217055189, 'num_leaves': 10, 'feature_fraction': 0.7860532342265497, 'bagging_fraction': 0.7848290998693316}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:18,999] Trial 63 finished with value: 0.9032862275449101 and parameters: {'lambda_l2': 6.548479577571958, 'num_leaves': 30, 'feature_fraction': 0.8240542922101358, 'bagging_fraction': 0.6547271650531814}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:19,261] Trial 64 finished with value: 0.9011952095808384 and parameters: {'lambda_l2': 5.106421668876233, 'num_leaves': 39, 'feature_fraction': 0.8859568466376256, 'bagging_fraction': 0.6989226270827487}. Best is trial 45 with value: 0.9057269461077845.\n",
      "[I 2024-05-19 17:14:19,448] Trial 65 finished with value: 0.9011389221556885 and parameters: {'lambda_l2': 5.604916327427379, 'num_leaves': 19, 'feature_fraction': 0.849453033797029, 'bagging_fraction': 0.7423520760613753}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:19,826] Trial 66 finished with value: 0.903348502994012 and parameters: {'lambda_l2': 6.0505862977135445, 'num_leaves': 52, 'feature_fraction': 0.794680829315184, 'bagging_fraction': 0.7723295976911753}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:20,204] Trial 67 finished with value: 0.9005556886227544 and parameters: {'lambda_l2': 7.433646913619265, 'num_leaves': 73, 'feature_fraction': 0.7355762186153976, 'bagging_fraction': 0.8295614029041634}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001910 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:20,459] Trial 68 finished with value: 0.9020443113772455 and parameters: {'lambda_l2': 6.743004069850853, 'num_leaves': 37, 'feature_fraction': 0.8307735633653851, 'bagging_fraction': 0.7971646557555221}. Best is trial 45 with value: 0.9057269461077845.\n",
      "[I 2024-05-19 17:14:20,669] Trial 69 finished with value: 0.9028994011976048 and parameters: {'lambda_l2': 4.775842241553198, 'num_leaves': 25, 'feature_fraction': 0.8800471597960211, 'bagging_fraction': 0.4524901610853945}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002472 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:21,009] Trial 70 finished with value: 0.9029580838323352 and parameters: {'lambda_l2': 1.195659380807296, 'num_leaves': 44, 'feature_fraction': 0.9112451425407802, 'bagging_fraction': 0.6826095639924785}. Best is trial 45 with value: 0.9057269461077845.\n",
      "[I 2024-05-19 17:14:21,165] Trial 71 finished with value: 0.8952209580838323 and parameters: {'lambda_l2': 3.4720844355835614, 'num_leaves': 6, 'feature_fraction': 0.973994083182282, 'bagging_fraction': 0.9141873315735434}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002559 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:21,403] Trial 72 finished with value: 0.9030970059880239 and parameters: {'lambda_l2': 3.112774448642687, 'num_leaves': 32, 'feature_fraction': 0.8558468393963696, 'bagging_fraction': 0.933262089994255}. Best is trial 45 with value: 0.9057269461077845.\n",
      "[I 2024-05-19 17:14:21,589] Trial 73 finished with value: 0.9018335329341317 and parameters: {'lambda_l2': 4.3083820693348525, 'num_leaves': 19, 'feature_fraction': 0.9355100456606054, 'bagging_fraction': 0.8858307846302439}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:21,930] Trial 74 finished with value: 0.8990886227544911 and parameters: {'lambda_l2': 3.5904974634623192, 'num_leaves': 51, 'feature_fraction': 0.42336464479867475, 'bagging_fraction': 0.8487639395903571}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:22,293] Trial 75 finished with value: 0.9026251497005988 and parameters: {'lambda_l2': 2.6640201014107427, 'num_leaves': 28, 'feature_fraction': 0.9030903811601106, 'bagging_fraction': 0.9702298167770427}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:22,672] Trial 76 finished with value: 0.9012586826347306 and parameters: {'lambda_l2': 2.201061495546378, 'num_leaves': 65, 'feature_fraction': 0.9287313664450813, 'bagging_fraction': 0.6176770427948598}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:22,934] Trial 77 finished with value: 0.9029293413173652 and parameters: {'lambda_l2': 4.203060348665623, 'num_leaves': 39, 'feature_fraction': 0.7966915613949055, 'bagging_fraction': 0.7191213856878543}. Best is trial 45 with value: 0.9057269461077845.\n",
      "[I 2024-05-19 17:14:23,094] Trial 78 finished with value: 0.9005461077844313 and parameters: {'lambda_l2': 5.896776940149259, 'num_leaves': 12, 'feature_fraction': 0.8370263906997754, 'bagging_fraction': 0.6463548552367183}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:23,554] Trial 79 finished with value: 0.9019137724550899 and parameters: {'lambda_l2': 5.145471916201831, 'num_leaves': 57, 'feature_fraction': 0.8131566103333675, 'bagging_fraction': 0.663866227499818}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:24,149] Trial 80 finished with value: 0.8972383233532933 and parameters: {'lambda_l2': 5.568045871999502, 'num_leaves': 130, 'feature_fraction': 0.764168244404474, 'bagging_fraction': 0.8596132271401886}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:24,459] Trial 81 finished with value: 0.9018131736526945 and parameters: {'lambda_l2': 6.42456319770668, 'num_leaves': 21, 'feature_fraction': 0.7411634029573227, 'bagging_fraction': 0.6296315078078302}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:24,687] Trial 82 finished with value: 0.9035029940119761 and parameters: {'lambda_l2': 5.806335956497043, 'num_leaves': 30, 'feature_fraction': 0.7844598694183647, 'bagging_fraction': 0.6926951862145567}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:25,024] Trial 83 finished with value: 0.9010095808383232 and parameters: {'lambda_l2': 4.501834371728608, 'num_leaves': 34, 'feature_fraction': 0.7819321297794127, 'bagging_fraction': 0.6937112748274975}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:25,312] Trial 84 finished with value: 0.9013748502994011 and parameters: {'lambda_l2': 8.136215922630468, 'num_leaves': 45, 'feature_fraction': 0.8481376198709235, 'bagging_fraction': 0.7407940454264504}. Best is trial 45 with value: 0.9057269461077845.\n",
      "[I 2024-05-19 17:14:25,408] Trial 85 finished with value: 0.863962874251497 and parameters: {'lambda_l2': 5.362165271193741, 'num_leaves': 2, 'feature_fraction': 0.8226099201562632, 'bagging_fraction': 0.6683098586550114}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:25,696] Trial 86 finished with value: 0.9018646706586826 and parameters: {'lambda_l2': 7.8072680762824795, 'num_leaves': 15, 'feature_fraction': 0.7975787786014591, 'bagging_fraction': 0.5973332568677987}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001938 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:25,925] Trial 87 finished with value: 0.9026922155688623 and parameters: {'lambda_l2': 4.810020341377632, 'num_leaves': 29, 'feature_fraction': 0.6985304745020802, 'bagging_fraction': 0.8865974391840175}. Best is trial 45 with value: 0.9057269461077845.\n",
      "[I 2024-05-19 17:14:26,114] Trial 88 finished with value: 0.8962395209580838 and parameters: {'lambda_l2': 6.223754717041354, 'num_leaves': 9, 'feature_fraction': 0.8712885930104659, 'bagging_fraction': 0.8226805364890654}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002804 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:26,980] Trial 89 finished with value: 0.8971365269461078 and parameters: {'lambda_l2': 8.954092895889564, 'num_leaves': 181, 'feature_fraction': 0.8360302913926734, 'bagging_fraction': 0.7047585295835276}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005854 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:27,268] Trial 90 finished with value: 0.9021556886227544 and parameters: {'lambda_l2': 4.044834888655337, 'num_leaves': 47, 'feature_fraction': 0.8084553558641994, 'bagging_fraction': 0.6362628088772314}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:27,528] Trial 91 finished with value: 0.9022634730538922 and parameters: {'lambda_l2': 5.816120090808618, 'num_leaves': 22, 'feature_fraction': 0.7702154391372537, 'bagging_fraction': 0.6550471726147565}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:27,787] Trial 92 finished with value: 0.9027952095808385 and parameters: {'lambda_l2': 6.9709530590759385, 'num_leaves': 37, 'feature_fraction': 0.7484819973929969, 'bagging_fraction': 0.687865140636683}. Best is trial 45 with value: 0.9057269461077845.\n",
      "[I 2024-05-19 17:14:28,016] Trial 93 finished with value: 0.9046586826347305 and parameters: {'lambda_l2': 5.182502663202751, 'num_leaves': 28, 'feature_fraction': 0.7266777819524458, 'bagging_fraction': 0.6633379383595613}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:28,294] Trial 94 finished with value: 0.903785628742515 and parameters: {'lambda_l2': 5.230083472920513, 'num_leaves': 41, 'feature_fraction': 0.7249022526520333, 'bagging_fraction': 0.7308574378102726}. Best is trial 45 with value: 0.9057269461077845.\n",
      "[I 2024-05-19 17:14:28,534] Trial 95 finished with value: 0.9026586826347305 and parameters: {'lambda_l2': 5.2224387188836054, 'num_leaves': 29, 'feature_fraction': 0.6486108360806846, 'bagging_fraction': 0.7602436473460522}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:28,776] Trial 96 finished with value: 0.9027640718562873 and parameters: {'lambda_l2': 4.971191636949622, 'num_leaves': 18, 'feature_fraction': 0.7256638320121234, 'bagging_fraction': 0.7393109761885411}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012689 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:29,063] Trial 97 finished with value: 0.9025568862275448 and parameters: {'lambda_l2': 5.558759137907634, 'num_leaves': 41, 'feature_fraction': 0.7101922253264326, 'bagging_fraction': 0.6059955994339427}. Best is trial 45 with value: 0.9057269461077845.\n",
      "[I 2024-05-19 17:14:29,285] Trial 98 finished with value: 0.9019604790419163 and parameters: {'lambda_l2': 4.683560623206253, 'num_leaves': 25, 'feature_fraction': 0.6850558009748444, 'bagging_fraction': 0.7257465874829259}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005627 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Number of data points in the train set: 37338, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 0.526515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 17:14:29,600] Trial 99 finished with value: 0.9019628742514969 and parameters: {'lambda_l2': 4.415772892909616, 'num_leaves': 48, 'feature_fraction': 0.6097241314823667, 'bagging_fraction': 0.7090633218061947}. Best is trial 45 with value: 0.9057269461077845.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lambda_l2': 4.768530754185674,\n",
       " 'num_leaves': 34,\n",
       " 'feature_fraction': 0.8205156123700014,\n",
       " 'bagging_fraction': 0.6303534743529644}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9057269461077845"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 贝叶斯搜索\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.01, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "    }\n",
    "    \n",
    "    model = lightgbm.train(params= param, train_set= train, valid_sets= test) # 建模\n",
    "    AUC= roc_auc_score(Ytest, model.predict(Xtest))   # matric\n",
    "    return AUC\n",
    "\n",
    "lgb_opt = optuna.create_study(direction='maximize')\n",
    "lgb_opt.optimize(objective, n_trials=100)  # 100次迭代\n",
    "\n",
    "# 打印结果\n",
    "lgb_opt.best_params\n",
    "lgb_opt.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be254e1",
   "metadata": {},
   "source": [
    "# Manifold Learning（t-SNE）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cf2c0f",
   "metadata": {},
   "source": [
    "当我们需要挖掘数据中潜在的低维/局部结构，可以考虑使用Manifold Learning方法进行降维，再训练分类器。大多数情况下我们选用t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20e41b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.19 GiB for an array with shape (697044453,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[0;32m      4\u001b[0m Xembedded \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m, perplexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, method\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexact\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m X_tsne\u001b[38;5;241m=\u001b[39m Xembedded\u001b[38;5;241m.\u001b[39mfit_transform(Xtrain)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1111\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;124;03m    Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m-> 1111\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:916\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m    913\u001b[0m     distances \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;66;03m# compute the joint probability distribution for the input space\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m P \u001b[38;5;241m=\u001b[39m _joint_probabilities(distances, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(np\u001b[38;5;241m.\u001b[39misfinite(P)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll probabilities should be finite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(P \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll probabilities should be non-negative\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:69\u001b[0m, in \u001b[0;36m_joint_probabilities\u001b[1;34m(distances, desired_perplexity, verbose)\u001b[0m\n\u001b[0;32m     67\u001b[0m P \u001b[38;5;241m=\u001b[39m conditional_P \u001b[38;5;241m+\u001b[39m conditional_P\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     68\u001b[0m sum_P \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(np\u001b[38;5;241m.\u001b[39msum(P), MACHINE_EPSILON)\n\u001b[1;32m---> 69\u001b[0m P \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(squareform(P) \u001b[38;5;241m/\u001b[39m sum_P, MACHINE_EPSILON)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m P\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.19 GiB for an array with shape (697044453,) and data type float64"
     ]
    }
   ],
   "source": [
    "# t-SNE的计算复杂度很高，非常耗时\n",
    "%time\n",
    "from sklearn.manifold import TSNE\n",
    "Xembedded = TSNE(n_components=10, learning_rate='auto', init='random', perplexity=30, method= \"exact\", n_jobs=-1)\n",
    "X_tsne= Xembedded.fit_transform(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a60c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
