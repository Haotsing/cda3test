{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化环境 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 对于高版本python（3.11）需要高版本spark与之配套\n",
    "#os.environ[\"SPARK_HOME\"] = R\"F:\\spark_local\\spark-3.4.2-bin-hadoop3\"\n",
    "#os.environ[\"HADOOP_HOME\"] = R\"F:\\spark_local\\hadoop-3.3.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "         SparkSession\n",
    "         .builder\n",
    "         .master(\"local[4]\") \n",
    "         .appName(\"SimpleApp\")\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sparkSQL 中的DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用内置函数创建 spark.range(start,end,step)  数据区间[start,end) ,start默认是0  step 默认是1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 由其他数据对象转换成DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.createDataFrame(data,schema)\n",
    "- data 数据：列表、RDD、pandas的dataframe都可以\n",
    "- schema ：元数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [[\"张三\",\"男\",89],[\"李四\",\"男\",99],[\"王五\",\"女\",8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|姓名|性别|score|\n",
      "+----+----+-----+\n",
      "|张三|  男|   89|\n",
      "|李四|  男|   99|\n",
      "|王五|  女|    8|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string_schema = \"\"\"\n",
    "`姓名` string\n",
    ",`性别` string\n",
    ",score int\n",
    "\"\"\"\n",
    "# 中文的字段名，或者别名需用``上引号（tab键上面的）括起来\n",
    "spark.createDataFrame(scores,schema=string_schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "rdd1=sc.parallelize([[\"张三\",\"男\",89],[\"李四\",\"男\",99],[\"王五\",\"女\",8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|姓名|性别|score|\n",
      "+----+----+-----+\n",
      "|张三|  男|   89|\n",
      "|李四|  男|   99|\n",
      "|王五|  女|    8|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(rdd1,schema=string_schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>姓名</th>\n",
       "      <th>性别</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>张三</td>\n",
       "      <td>男</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>李四</td>\n",
       "      <td>男</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>王五</td>\n",
       "      <td>女</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   姓名 性别  score\n",
       "0  张三  男     89\n",
       "1  李四  男     99\n",
       "2  王五  女      8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(scores,columns=[\"姓名\",\"性别\",\"score\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|姓名|性别|score|\n",
      "+----+----+-----+\n",
      "|张三|  男|   89|\n",
      "|李四|  男|   99|\n",
      "|王五|  女|    8|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过数据文件创建DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 文本文件CSV\n",
    "moviesDF = spark.read.csv(\"./data/movies.csv\",header=True)\n",
    "moviesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------+\n",
      "|       actor_name|       movie_title|produced_year|\n",
      "+-----------------+------------------+-------------+\n",
      "|McClure, Marc (I)|      Coach Carter|         2005|\n",
      "|McClure, Marc (I)|       Superman II|         1980|\n",
      "|McClure, Marc (I)|         Apollo 13|         1995|\n",
      "|McClure, Marc (I)|          Superman|         1978|\n",
      "|McClure, Marc (I)|Back to the Future|         1985|\n",
      "+-----------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parquet 列式存储文件，每个列的数据单独存储成一个文件\n",
    "movies_parquet = spark.read.parquet(\"./data/movies.parquet\")\n",
    "movies_parquet.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要在spark的jars文件夹放入驱动程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"jdbc:mysql://localhost:3306/cda?user=root&password=112233\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+--------+----------+----------+---------+----+----+--------+\n",
      "|              合同ID|  客户编号|客户名称|合同期限|  放款日期|  到期时间| 合同金额|性别|年龄|婚姻状况|\n",
      "+--------------------+----------+--------+--------+----------+----------+---------+----+----+--------+\n",
      "|0005f28cc32945ecb...|1808067960|    王倩|      18|2018-08-08|2020-02-16| 80000.00|  女|  33|    已婚|\n",
      "|000d5719bfee496c8...|1711231404|  景金凤|       6|2017-11-23|2018-05-23| 36000.00|  女|  42|    已婚|\n",
      "|002837ccb279426fa...|1809259131|  王理传|      12|2018-09-26|2019-09-28| 68000.00|  男|  43|    已婚|\n",
      "|006a6fea8cde4e9bb...|1802064089|  林绍监|      12|2018-02-06|2019-02-06|272000.00|  男|  43|    已婚|\n",
      "|007075347e204f339...|1806195568|  彭晓军|       6|2018-06-20|2018-12-20| 86000.00|  男|  44|    已婚|\n",
      "|008639881df24730b...|1803224620|  梁应龙|       6|2018-03-23|2018-09-23|113000.00|  男|  53|    已婚|\n",
      "|0124bdb32b614ec3a...|1712072461|    王霞|       6|2018-06-13|2018-12-13|204000.00|  女|  36|    已婚|\n",
      "|0140ab0461bf4353a...|1806255706|  李勇军|       6|2018-06-28|2018-12-28|168000.00|  男|  38|    已婚|\n",
      "|017d7385ebd64ca9b...|1807236993|  李伟鸿|       3|2018-07-24|2018-10-27| 92000.00|  男|  28|    已婚|\n",
      "|018334e740c64ac3b...|1803034267|  黄玉金|      12|2018-08-31|2019-08-28|100000.00|  男|  44|    已婚|\n",
      "+--------------------+----------+--------+--------+----------+----------+---------+----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_detail = (spark.read\n",
    "                     .format(\"jdbc\")\n",
    "                     .option(\"url\",url)\n",
    "                     .option(\"query\",\"select * from customer_detail limit 10\")\n",
    "                     .load()\n",
    "                    )\n",
    "customer_detail.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 也可以写数据库\n",
    "(customer_detail.write\n",
    ".mode(\"append\")\n",
    ".format(\"jdbc\")\n",
    ".option(\"url\",url)\n",
    ".option(\"dbtable\",\"contact\")\n",
    ".save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(*)|\n",
      "+--------+\n",
      "|      30|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " (spark.read\n",
    "         .format(\"jdbc\")\n",
    "         .option(\"url\",url)\n",
    "         .option(\"query\",\"select count(*) from contact\")\n",
    "         .load()\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame API\n",
    "\n",
    "与RDD类似，DataFrame的API也分为两类：transformation和action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选择列select  selectExpr\n",
    "select与sql中的类似，选取指定列（指定条件的数据）\n",
    "\n",
    "\n",
    "\n",
    "selectExpr与select 类似，不同的是可以解析表达式（底层进行了SQL转化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_name: string (nullable = true)\n",
      " |-- movie_title: string (nullable = true)\n",
      " |-- produced_year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------+--------------------+\n",
      "|       actor_name|         movie_title|produced_year|         movie_title|\n",
      "+-----------------+--------------------+-------------+--------------------+\n",
      "|McClure, Marc (I)|        Coach Carter|         2005|        Coach Carter|\n",
      "|McClure, Marc (I)|         Superman II|         1980|         Superman II|\n",
      "|McClure, Marc (I)|           Apollo 13|         1995|           Apollo 13|\n",
      "|McClure, Marc (I)|            Superman|         1978|            Superman|\n",
      "|McClure, Marc (I)|  Back to the Future|         1985|  Back to the Future|\n",
      "|McClure, Marc (I)|Back to the Futur...|         1990|Back to the Futur...|\n",
      "|Cooper, Chris (I)|  Me, Myself & Irene|         2000|  Me, Myself & Irene|\n",
      "|Cooper, Chris (I)|         October Sky|         1999|         October Sky|\n",
      "|Cooper, Chris (I)|              Capote|         2005|              Capote|\n",
      "|Cooper, Chris (I)|The Bourne Supremacy|         2004|The Bourne Supremacy|\n",
      "|Cooper, Chris (I)|         The Patriot|         2000|         The Patriot|\n",
      "|Cooper, Chris (I)|            The Town|         2010|            The Town|\n",
      "|Cooper, Chris (I)|          Seabiscuit|         2003|          Seabiscuit|\n",
      "|Cooper, Chris (I)|      A Time to Kill|         1996|      A Time to Kill|\n",
      "|Cooper, Chris (I)|Where the Wild Th...|         2009|Where the Wild Th...|\n",
      "|Cooper, Chris (I)|         The Muppets|         2011|         The Muppets|\n",
      "|Cooper, Chris (I)|     American Beauty|         1999|     American Beauty|\n",
      "|Cooper, Chris (I)|             Syriana|         2005|             Syriana|\n",
      "|Cooper, Chris (I)| The Horse Whisperer|         1998| The Horse Whisperer|\n",
      "|Cooper, Chris (I)|             Jarhead|         2005|             Jarhead|\n",
      "+-----------------+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_parquet.select(\"*\",\"movie_title\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `produced_year - produced_year%10` cannot be resolved. Did you mean one of the following? [`produced_year`, `actor_name`, `movie_title`].;\n'Project [movie_title#107, 'produced_year - produced_year%10]\n+- Relation [actor_name#106,movie_title#107,produced_year#108L] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 查询电影的上映年代\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#select 不能放字符串表达式\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 这里会报错，需要用selectExpr\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m movies_parquet\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovie_title\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduced_year - produced_year\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m10\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32mC:\\spark_setup\\spark-3.4.2-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:3036\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   2992\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   2993\u001b[0m \n\u001b[0;32m   2994\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3034\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[0;32m   3035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3036\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jcols(\u001b[38;5;241m*\u001b[39mcols))\n\u001b[0;32m   3037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mC:\\spark_setup\\spark-3.4.2-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark_setup\\spark-3.4.2-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `produced_year - produced_year%10` cannot be resolved. Did you mean one of the following? [`produced_year`, `actor_name`, `movie_title`].;\n'Project [movie_title#107, 'produced_year - produced_year%10]\n+- Relation [actor_name#106,movie_title#107,produced_year#108L] parquet\n"
     ]
    }
   ],
   "source": [
    "# 查询电影的上映年代\n",
    "#select 不能放字符串表达式\n",
    "# 这里会报错，需要用selectExpr\n",
    "movies_parquet.select(\"movie_title\",\"produced_year - produced_year%10\").show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------------------------+\n",
      "|       movie_title|(produced_year - (produced_year % 10))|\n",
      "+------------------+--------------------------------------+\n",
      "|      Coach Carter|                                  2000|\n",
      "|       Superman II|                                  1980|\n",
      "|         Apollo 13|                                  1990|\n",
      "|          Superman|                                  1970|\n",
      "|Back to the Future|                                  1980|\n",
      "+------------------+--------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_parquet.selectExpr(\"movie_title\",\"produced_year - produced_year%10\").show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------+\n",
      "|       actor_name|       movie_title|produced_year|\n",
      "+-----------------+------------------+-------------+\n",
      "|McClure, Marc (I)|      Coach Carter|         2005|\n",
      "|McClure, Marc (I)|       Superman II|         1980|\n",
      "|McClure, Marc (I)|         Apollo 13|         1995|\n",
      "|McClure, Marc (I)|          Superman|         1978|\n",
      "|McClure, Marc (I)|Back to the Future|         1985|\n",
      "+-----------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用*查询所有字段\n",
    "movies_parquet.selectExpr(\"*\").show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 过滤行 filter, where\n",
    "\n",
    "filter where 用来设置条件筛选字段，两者用法一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------+\n",
      "|         actor_name|         movie_title|produced_year|\n",
      "+-------------------+--------------------+-------------+\n",
      "|  McClure, Marc (I)|         Superman II|         1980|\n",
      "|  McClure, Marc (I)|           Apollo 13|         1995|\n",
      "|  McClure, Marc (I)|            Superman|         1978|\n",
      "|  McClure, Marc (I)|  Back to the Future|         1985|\n",
      "|  McClure, Marc (I)|Back to the Futur...|         1990|\n",
      "|  Cooper, Chris (I)|         October Sky|         1999|\n",
      "|  Cooper, Chris (I)|      A Time to Kill|         1996|\n",
      "|  Cooper, Chris (I)|     American Beauty|         1999|\n",
      "|  Cooper, Chris (I)| The Horse Whisperer|         1998|\n",
      "|Knight, Shirley (I)|  As Good as It Gets|         1997|\n",
      "|    Jolie, Angelina|  The Bone Collector|         1999|\n",
      "|     Danner, Blythe|    Forces of Nature|         1999|\n",
      "|     Danner, Blythe|         The X Files|         1998|\n",
      "|  Bissonnette, Joel|          Fight Club|         1999|\n",
      "|      Butters, Mike|        Dante's Peak|         1997|\n",
      "|      Butters, Mike|             Titanic|         1997|\n",
      "|      Butters, Mike|D2: The Mighty Ducks|         1994|\n",
      "|     Ruskin, Joseph|   Indecent Proposal|         1993|\n",
      "|     Ruskin, Joseph|Star Trek: Insurr...|         1998|\n",
      "|  Jennings, Juanita|      Basic Instinct|         1992|\n",
      "+-------------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 选择2000年以前电影\n",
    "movies_parquet.where(\"produced_year<2000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不等比较使用的操作符是 !="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------+\n",
      "|       actor_name|         movie_title|produced_year|\n",
      "+-----------------+--------------------+-------------+\n",
      "|McClure, Marc (I)|        Coach Carter|         2005|\n",
      "|McClure, Marc (I)|         Superman II|         1980|\n",
      "|McClure, Marc (I)|           Apollo 13|         1995|\n",
      "|McClure, Marc (I)|            Superman|         1978|\n",
      "|McClure, Marc (I)|  Back to the Future|         1985|\n",
      "|McClure, Marc (I)|Back to the Futur...|         1990|\n",
      "|Cooper, Chris (I)|         October Sky|         1999|\n",
      "|Cooper, Chris (I)|              Capote|         2005|\n",
      "|Cooper, Chris (I)|The Bourne Supremacy|         2004|\n",
      "|Cooper, Chris (I)|            The Town|         2010|\n",
      "|Cooper, Chris (I)|          Seabiscuit|         2003|\n",
      "|Cooper, Chris (I)|      A Time to Kill|         1996|\n",
      "|Cooper, Chris (I)|Where the Wild Th...|         2009|\n",
      "|Cooper, Chris (I)|         The Muppets|         2011|\n",
      "|Cooper, Chris (I)|     American Beauty|         1999|\n",
      "|Cooper, Chris (I)|             Syriana|         2005|\n",
      "|Cooper, Chris (I)| The Horse Whisperer|         1998|\n",
      "|Cooper, Chris (I)|             Jarhead|         2005|\n",
      "|Cooper, Chris (I)| The Bourne Identity|         2002|\n",
      "|Cassavetes, Frank|          Battleship|         2012|\n",
      "+-----------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 非 2000年的\n",
    "movies_parquet.where(\"produced_year!=2000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "组合一个或多个比较表达式, 我们将使用OR和AND表达式运算符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------+\n",
      "|         actor_name|         movie_title|produced_year|\n",
      "+-------------------+--------------------+-------------+\n",
      "|  Cooper, Chris (I)|The Bourne Supremacy|         2004|\n",
      "|  Cooper, Chris (I)|          Seabiscuit|         2003|\n",
      "|  Cooper, Chris (I)| The Bourne Identity|         2002|\n",
      "|  Cassavetes, Frank|              John Q|         2002|\n",
      "|Knight, Shirley (I)|Divine Secrets of...|         2002|\n",
      "|    Jolie, Angelina|Lara Croft: Tomb ...|         2001|\n",
      "|    Jolie, Angelina|           Alexander|         2004|\n",
      "|    Jolie, Angelina|          Shark Tale|         2004|\n",
      "|    Jolie, Angelina|Lara Croft Tomb R...|         2003|\n",
      "|    Jolie, Angelina|Sky Captain and t...|         2004|\n",
      "|     Yip, Françoise|      Blade: Trinity|         2004|\n",
      "|     Cueto, Esteban|   Collateral Damage|         2002|\n",
      "|     Cueto, Esteban|   The Scorpion King|         2002|\n",
      "|     Cueto, Esteban|                 xXx|         2002|\n",
      "|     Danner, Blythe|    Meet the Fockers|         2004|\n",
      "|     Danner, Blythe|Hauru no ugoku shiro|         2004|\n",
      "|           Buck (X)|           Snow Dogs|         2002|\n",
      "|  Bissonnette, Joel|The Sum of All Fears|         2002|\n",
      "|      Butters, Mike|                 Saw|         2004|\n",
      "|   Taylor, Elayn J.|      Dr. Dolittle 2|         2001|\n",
      "+-------------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2000年到2005年之间的电影\n",
    "movies_parquet.where(\"produced_year>2000 and produced_year<2005\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distinct, dropDuplicates\n",
    "去除重复行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|count(movie_title)|\n",
      "+------------------+\n",
      "|             31393|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|count(movie_title)|\n",
      "+------------------+\n",
      "|              1409|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 电影数量\n",
    "movies_parquet.selectExpr('count(movie_title)').show()\n",
    "# 去重后的数量\n",
    "movies_parquet.select('movie_title').distinct().selectExpr('count(movie_title)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|count(movie_title)|\n",
      "+------------------+\n",
      "|              1409|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_parquet.dropDuplicates([\"movie_title\"]).selectExpr('count(movie_title)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort(columns), orderBy(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------+-----+\n",
      "|          actor_name|movie_title|produced_year|t_len|\n",
      "+--------------------+-----------+-------------+-----+\n",
      "|    Efremov, Mikhail|         12|         2007|    2|\n",
      "|      Franko, Victor|         21|         2008|    2|\n",
      "|   Baessato, Giacomo|         RV|         2006|    2|\n",
      "|     Newman, Laraine|         Up|         2009|    2|\n",
      "|      Hayley, Robert|         X2|         2003|    2|\n",
      "|       Schaap, David|        300|         2006|    3|\n",
      "|        Cray, Ed (I)|        Ali|         2001|    3|\n",
      "|   LaMarche, Maurice|        Elf|         2003|    3|\n",
      "|Fay, Zachary Chri...|        Hop|         2011|    3|\n",
      "|  Herthum, Harold G.|        JFK|         1991|    3|\n",
      "|     Slattery, Cindy|        Rio|         2011|    3|\n",
      "|       Butters, Mike|        Saw|         2004|    3|\n",
      "|      Cueto, Esteban|        xXx|         2002|    3|\n",
      "|  Jackson, Samuel L.|       1408|         2007|    4|\n",
      "|        Allen, Woody|       Antz|         1998|    4|\n",
      "| Cromwell, James (I)|       Babe|         1995|    4|\n",
      "|    Drake, Larry (I)|       Bean|         1997|    4|\n",
      "|     King, Jaime (I)|       Blow|         2001|    4|\n",
      "|Anderson, Stephen...|       Bolt|         2008|    4|\n",
      "|      Carlin, George|       Cars|         2006|    4|\n",
      "+--------------------+-----------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 根据电影title名称长短排序\n",
    " # 默认升序排序\n",
    "(movies_parquet\n",
    " .dropDuplicates([\"movie_title\"])\n",
    " .selectExpr(\"*\",\"length(movie_title) t_len\")\n",
    " .sort(\"t_len\") \n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+-----+\n",
      "|          actor_name|         movie_title|produced_year|t_len|\n",
      "+--------------------+--------------------+-------------+-----+\n",
      "|             Luenell|Borat: Cultural L...|         2006|   83|\n",
      "|Moseley, William (I)|The Chronicles of...|         2005|   62|\n",
      "|        Cyrus, Miley|Hannah Montana & ...|         2008|   57|\n",
      "|      Galtsev, Yuriy|Istoriya pro Rich...|         1997|   56|\n",
      "|Moseley, William (I)|The Chronicles of...|         2010|   56|\n",
      "|     Aduramo, Israel|Pirates of the Ca...|         2003|   54|\n",
      "|     Newton, Thandie|Interview with th...|         1994|   50|\n",
      "|        Roehm, Jason|Indiana Jones and...|         2008|   50|\n",
      "|      Hayley, Robert|Percy Jackson & t...|         2010|   50|\n",
      "|        Grieve, Phil|The Lord of the R...|         2001|   49|\n",
      "|Bonham Carter, He...|Wallace & Gromit ...|         2005|   48|\n",
      "|    Pugh, Robert (I)|Master and Comman...|         2003|   47|\n",
      "| Browning, Emily (I)|Lemony Snicket's ...|         2004|   47|\n",
      "|       Prowse, David|Star Wars: Episod...|         1980|   46|\n",
      "|   Corbett, John (I)|Magnificent Desol...|         2005|   46|\n",
      "|Bonham Carter, He...|Sweeney Todd: The...|         2007|   46|\n",
      "|Guest, Christophe...|Night at the Muse...|         2009|   46|\n",
      "|     Englund, Robert|A Nightmare on El...|         1988|   45|\n",
      "|Jackson, Billy (III)|The Lord of the R...|         2003|   45|\n",
      "|    Hunter, Bill (I)|Legend of the Gua...|         2010|   45|\n",
      "+--------------------+--------------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用sparkSQL函数实现升降的设定\n",
    "from pyspark.sql.functions import desc,asc\n",
    "(movies_parquet\n",
    " .dropDuplicates([\"movie_title\"])\n",
    " .selectExpr(\"*\",\"length(movie_title) t_len\")\n",
    " .sort(desc(\"t_len\"),asc(\"produced_year\")) \n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### limit(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+-------------+\n",
      "|       actor_name| movie_title|produced_year|\n",
      "+-----------------+------------+-------------+\n",
      "|McClure, Marc (I)|Coach Carter|         2005|\n",
      "|McClure, Marc (I)| Superman II|         1980|\n",
      "|McClure, Marc (I)|   Apollo 13|         1995|\n",
      "+-----------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 选择前3行\n",
    "movies_parquet.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+-------------+\n",
      "|       actor_name| movie_title|produced_year|\n",
      "+-----------------+------------+-------------+\n",
      "|McClure, Marc (I)|Coach Carter|         2005|\n",
      "|McClure, Marc (I)| Superman II|         1980|\n",
      "|McClure, Marc (I)|   Apollo 13|         1995|\n",
      "+-----------------+------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 显示前3行\n",
    "movies_parquet.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### union(otherDataFrame)\n",
    "相当于SQL的`UNION ALL`,去重用前面的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col0|col1|col2|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+\n",
      "|col1|col2|col0|\n",
      "+----+----+----+\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
    "df1.show()\n",
    "df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col0|col1|col2|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   1|   2|   3|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+\n",
      "|col0|col1|col2|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df1).show()\n",
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### withColumn(colName, column)\n",
    "向DataFrame增加一个新的列    \n",
    "增加一个新列，基于某一列表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------+----+\n",
      "|       actor_name|         movie_title|produced_year|年代|\n",
      "+-----------------+--------------------+-------------+----+\n",
      "|McClure, Marc (I)|        Coach Carter|         2005|2000|\n",
      "|McClure, Marc (I)|         Superman II|         1980|1980|\n",
      "|McClure, Marc (I)|           Apollo 13|         1995|1990|\n",
      "|McClure, Marc (I)|            Superman|         1978|1970|\n",
      "|McClure, Marc (I)|  Back to the Future|         1985|1980|\n",
      "|McClure, Marc (I)|Back to the Futur...|         1990|1990|\n",
      "|Cooper, Chris (I)|  Me, Myself & Irene|         2000|2000|\n",
      "|Cooper, Chris (I)|         October Sky|         1999|1990|\n",
      "|Cooper, Chris (I)|              Capote|         2005|2000|\n",
      "|Cooper, Chris (I)|The Bourne Supremacy|         2004|2000|\n",
      "|Cooper, Chris (I)|         The Patriot|         2000|2000|\n",
      "|Cooper, Chris (I)|            The Town|         2010|2010|\n",
      "|Cooper, Chris (I)|          Seabiscuit|         2003|2000|\n",
      "|Cooper, Chris (I)|      A Time to Kill|         1996|1990|\n",
      "|Cooper, Chris (I)|Where the Wild Th...|         2009|2000|\n",
      "|Cooper, Chris (I)|         The Muppets|         2011|2010|\n",
      "|Cooper, Chris (I)|     American Beauty|         1999|1990|\n",
      "|Cooper, Chris (I)|             Syriana|         2005|2000|\n",
      "|Cooper, Chris (I)| The Horse Whisperer|         1998|1990|\n",
      "|Cooper, Chris (I)|             Jarhead|         2005|2000|\n",
      "+-----------------+--------------------+-------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_parquet.withColumn(\"年代\",movies_parquet.produced_year-movies_parquet.produced_year%10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### withColumnRenamed(existingColName, newColName)\n",
    "修改列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------+\n",
      "|            actor|       movie_title|produced_year|\n",
      "+-----------------+------------------+-------------+\n",
      "|McClure, Marc (I)|      Coach Carter|         2005|\n",
      "|McClure, Marc (I)|       Superman II|         1980|\n",
      "|McClure, Marc (I)|         Apollo 13|         1995|\n",
      "|McClure, Marc (I)|          Superman|         1978|\n",
      "|McClure, Marc (I)|Back to the Future|         1985|\n",
      "+-----------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_parquet.withColumnRenamed(\"actor_name\",\"actor\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop(columnName1, columnName2)\n",
    "删除指定的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|         movie_title|produced_year|\n",
      "+--------------------+-------------+\n",
      "|        Coach Carter|         2005|\n",
      "|         Superman II|         1980|\n",
      "|           Apollo 13|         1995|\n",
      "|            Superman|         1978|\n",
      "|  Back to the Future|         1985|\n",
      "|Back to the Futur...|         1990|\n",
      "|  Me, Myself & Irene|         2000|\n",
      "|         October Sky|         1999|\n",
      "|              Capote|         2005|\n",
      "|The Bourne Supremacy|         2004|\n",
      "|         The Patriot|         2000|\n",
      "|            The Town|         2010|\n",
      "|          Seabiscuit|         2003|\n",
      "|      A Time to Kill|         1996|\n",
      "|Where the Wild Th...|         2009|\n",
      "|         The Muppets|         2011|\n",
      "|     American Beauty|         1999|\n",
      "|             Syriana|         2005|\n",
      "| The Horse Whisperer|         1998|\n",
      "|             Jarhead|         2005|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_parquet.drop(\"actor_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dropna\n",
    "删除缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "badmovie = [[None,None,2005],\n",
    "           [\"Jarhead\",None,2005],\n",
    "           [None,\"The Horse Whisperer\",2005]]\n",
    "bad_df = spark.createDataFrame(badmovie,schema=movies_parquet.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+\n",
      "|actor_name|        movie_title|produced_year|\n",
      "+----------+-------------------+-------------+\n",
      "|      null|               null|         2005|\n",
      "|   Jarhead|               null|         2005|\n",
      "|      null|The Horse Whisperer|         2005|\n",
      "+----------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bad_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+\n",
      "|actor_name|        movie_title|produced_year|\n",
      "+----------+-------------------+-------------+\n",
      "|   Jarhead|               null|         2005|\n",
      "|      null|The Horse Whisperer|         2005|\n",
      "+----------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bad_df.dropna(how=\"all\",subset=[\"actor_name\",\"movie_title\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+\n",
      "|actor_name|        movie_title|produced_year|\n",
      "+----------+-------------------+-------------+\n",
      "|   Jarhead|               null|         2005|\n",
      "|      null|The Horse Whisperer|         2005|\n",
      "+----------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 非空数据大于等于2则不删除\n",
    "bad_df.dropna(thresh=2 # 非空数据至少有两个\n",
    "              ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分组聚合\n",
    "- groupBy传入分组的列字符串形式\n",
    "- agg 传入聚合表达式\n",
    "   - 例如字典形式{\"movie_title\":\"count\"} 对字段\"movie_title\"进行count计算\n",
    "   - 也可以使用内置函数 例如 F.count(movies_parquet.movie_title).alias(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|          actor_name|count(movie_title)|\n",
      "+--------------------+------------------+\n",
      "|    Branson, Richard|                 3|\n",
      "|      Craig, Georgia|                 3|\n",
      "|        Rossum, Emmy|                 4|\n",
      "|  Ironbear, Mylo (I)|                 3|\n",
      "|        Fiddy, James|                 6|\n",
      "|   Bosco, Philip (I)|                 5|\n",
      "|     Spencer, Chaske|                 3|\n",
      "|            Rule, Ja|                 3|\n",
      "|Bishop, Stephen (II)|                 7|\n",
      "|       Bannos, Steve|                 5|\n",
      "|       Lester, Loren|                 4|\n",
      "|      Mercurio, Tara|                 3|\n",
      "|     Abadie, William|                 3|\n",
      "|  Taylor, Sandra (I)|                 7|\n",
      "|MacKay, Michael Reid|                 4|\n",
      "|Nichols, Jonathan...|                 4|\n",
      "|         Krupa, Olek|                 9|\n",
      "|        Hunt, Bonnie|                12|\n",
      "|      Thornton, Kirk|                 3|\n",
      "|      Smigel, Robert|                 4|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_parquet.groupBy(\"actor_name\").agg({\"movie_title\":\"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|          actor_name|count(movie_title)|\n",
      "+--------------------+------------------+\n",
      "|    Branson, Richard|                 3|\n",
      "|      Craig, Georgia|                 3|\n",
      "|        Rossum, Emmy|                 4|\n",
      "|  Ironbear, Mylo (I)|                 3|\n",
      "|        Fiddy, James|                 6|\n",
      "|   Bosco, Philip (I)|                 5|\n",
      "|     Spencer, Chaske|                 3|\n",
      "|            Rule, Ja|                 3|\n",
      "|Bishop, Stephen (II)|                 7|\n",
      "|       Bannos, Steve|                 5|\n",
      "|       Lester, Loren|                 4|\n",
      "|      Mercurio, Tara|                 3|\n",
      "|     Abadie, William|                 3|\n",
      "|  Taylor, Sandra (I)|                 7|\n",
      "|MacKay, Michael Reid|                 4|\n",
      "|Nichols, Jonathan...|                 4|\n",
      "|         Krupa, Olek|                 9|\n",
      "|        Hunt, Bonnie|                12|\n",
      "|      Thornton, Kirk|                 3|\n",
      "|      Smigel, Robert|                 4|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F # 使用F中的count函数，这样返回dataframe 列对象，可以用alias设置别名\n",
    "movies_parquet.groupBy(\"actor_name\").agg(F.count(\"movie_title\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+------------------+\n",
      "|summary|        actor_name|         movie_title|     produced_year|\n",
      "+-------+------------------+--------------------+------------------+\n",
      "|  count|             31393|               31393|             31392|\n",
      "|   mean|              null|  312.61538461538464|2002.7964449541284|\n",
      "| stddev|              null|   485.7043414390151| 6.377236851493877|\n",
      "|    min|   Aaron, Caroline|'Crocodile' Dunde...|              1961|\n",
      "|    max|von Sydow, Max (I)|                 xXx|              2012|\n",
      "+-------+------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_parquet.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show\n",
    "\n",
    "查看数据可以使用show() 函数，这样程序会将数据打印到代码下方。\n",
    "- n 表示要展示的数据行数，默认20行\n",
    "- truncate 显示时列宽，默认最多显示20个字符，可以给整数设置其显示宽度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### head\n",
    "\n",
    "**返回**数据第一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(actor_name='McClure, Marc (I)', movie_title='Coach Carter', produced_year=2005)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_parquet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'McClure, Marc (I)'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_parquet.head()[\"actor_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(actor_name='McClure, Marc (I)', movie_title='Coach Carter', produced_year=2005)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_parquet.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(actor_name='McClure, Marc (I)', movie_title='Coach Carter', produced_year=2005),\n",
       " Row(actor_name='McClure, Marc (I)', movie_title='Superman II', produced_year=1980),\n",
       " Row(actor_name='McClure, Marc (I)', movie_title='Apollo 13', produced_year=1995)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_parquet.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect\n",
    "\n",
    "以列表形式返回到客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(actor_name='McClure, Marc (I)', movie_title='Coach Carter', produced_year=2005),\n",
       " Row(actor_name='McClure, Marc (I)', movie_title='Superman II', produced_year=1980),\n",
       " Row(actor_name='McClure, Marc (I)', movie_title='Apollo 13', produced_year=1995)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_parquet.limit(3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31393"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9742"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用SQL查询"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark提供的最酷功能之一是能够使用SQL来执行分布式数据操作或大规模数据分析\n",
    "\n",
    "<small><ul><li>Spark实现了ANSI SQL:2003修订版的一个子集</li><li>需要先将DataFrame注册为临时视图，然后再进行SQL查询</li><li>Spark为临时视图提供了两个级别的范围。一个是Spark会话级别。第二个作用域级别是全局级别的</li><li>所有已注册的视图都保存在Spark元数据目录中，可以通过SparkSession访问该目录</li><li>使用SparkSession类的sql函数执行SQL语句</li></ul></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### createOrReplaceTempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将df注册为临时视图 movies\n",
    "movies_parquet.createOrReplaceTempView(\"movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+-------------+\n",
      "|       actor_name| movie_title|produced_year|\n",
      "+-----------------+------------+-------------+\n",
      "|McClure, Marc (I)|Coach Carter|         2005|\n",
      "|McClure, Marc (I)| Superman II|         1980|\n",
      "|McClure, Marc (I)|   Apollo 13|         1995|\n",
      "+-----------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from movies limit 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 与API混用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------------+\n",
      "|          actor_name|numb|         avg_numb|\n",
      "+--------------------+----+-----------------+\n",
      "|        Fiddy, James|   6|4.809713497778459|\n",
      "|   Bosco, Philip (I)|   5|4.809713497778459|\n",
      "|Bishop, Stephen (II)|   7|4.809713497778459|\n",
      "|       Bannos, Steve|   5|4.809713497778459|\n",
      "|  Taylor, Sandra (I)|   7|4.809713497778459|\n",
      "|         Krupa, Olek|   9|4.809713497778459|\n",
      "|        Hunt, Bonnie|  12|4.809713497778459|\n",
      "|        Bibb, Leslie|   5|4.809713497778459|\n",
      "|   Smith, Maggie (I)|  16|4.809713497778459|\n",
      "|Harris, Neil Patrick|   6|4.809713497778459|\n",
      "|     Bacarella, Mike|   7|4.809713497778459|\n",
      "|   Olyphant, Timothy|   7|4.809713497778459|\n",
      "|    Derryberry, Debi|  18|4.809713497778459|\n",
      "| Williams, Robin (I)|  22|4.809713497778459|\n",
      "|     McCallany, Holt|   6|4.809713497778459|\n",
      "|        Dorff, Holly|   6|4.809713497778459|\n",
      "|  Martin, Andrea (I)|   6|4.809713497778459|\n",
      "|   Bridges, Jeff (I)|   7|4.809713497778459|\n",
      "|      Lindberg, Chad|   6|4.809713497778459|\n",
      "|      Cheung, George|  10|4.809713497778459|\n",
      "+--------------------+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 电影数量高于平均值的演员信息\n",
    "spark.sql(\"\"\"\n",
    "select actor_name\n",
    "      ,numb\n",
    "      ,avg(numb) over() avg_numb\n",
    "from (select actor_name\n",
    "          ,count(*) numb\n",
    "    from movies\n",
    "    group by actor_name) t\n",
    "\"\"\").where(\"numb>avg_numb\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常用函数\n",
    "\n",
    "pyspark.sql.functions提供的用于数值、字符串、日期的函数远不止PPT中罗列的这些，还有关于编码转换、进制转换、时区转换、数组转换、三角函数等函数。    \n",
    "\n",
    "当我们使用API的方法使用dataframe的时候需要使用从pyspark.sql.functions导入相关函数；    \n",
    "\n",
    "当我们以SQL的形式查询数据时，可以直接在SQL中使用这些函数。\n",
    "\n",
    "我们这里内部函数介绍一下时间相关函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: explode\n",
      "Class: org.apache.spark.sql.catalyst.expressions.Explode\n",
      "Usage: explode(expr) - Separates the elements of array `expr` into multiple rows, or the elements of map `expr` into multiple rows and columns. Unless specified otherwise, uses the default column name `col` for elements of the array or `key` and `value` for the elements of the map.\n",
      "Extended Usage:\n",
      "    Examples:\n",
      "      > SELECT explode(array(10, 20));\n",
      "       10\n",
      "       20\n",
      "  \n",
      "    Since: 1.0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"############ SparkSQL 函数列表 ############\")\n",
    "# spark.sql(\"show functions\").show(333,False)\n",
    "\n",
    "# 通过语法 desc function extended 函数名 查看函数文档\n",
    "# 自定义函数，以更好的格式查看文档信息\n",
    "\n",
    "def print_doc(func_name:str)->None:\n",
    "    for i in spark.sql(f\"desc function extended {func_name} \").head(10):\n",
    "        print(i[\"function_desc\"])\n",
    "\n",
    "print_doc(\"explode\")\n",
    "# spark.sql(\"show functions\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内置函数\n",
    "\n",
    "### 内置函数: 处理日期时间函数\n",
    "    \n",
    "Spark内置的日期时间函数大致可分为以下三个类别：\n",
    "- 1)将日期或时间戳从一种格式转换为另一种格式\n",
    "- 2)执行日期时间计算\n",
    "- 3)并从日期或时间戳中提取特定的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 字符串与时间（date,datetime,unix_timestamp） 互换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日期和时间转换函数：这些函数使用的默认的日期格式是yyyy-MM-dd HH:mm:ss    \n",
    "- to_timestamp, 转成datetime\n",
    "- to_date,     转成date\n",
    "- to_unix_timestamp/unix_timestamp  转换为整数 Unix 时间戳\n",
    "  - 注意  to_unix_timestamp 只能在SQL里面用，API上不行  unix_timestamp则在两边均可使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+----------+----------------+\n",
      "| id|      date|           datetime|  date_str|          dt_str|\n",
      "+---+----------+-------------------+----------+----------------+\n",
      "|  1|2023-05-28|2023-05-28 13:31:09|28-05-2023|28-05-2023 15:50|\n",
      "+---+----------+-------------------+----------+----------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- datetime: string (nullable = true)\n",
      " |-- date_str: string (nullable = true)\n",
      " |-- dt_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = [(1,'2023-05-28',\"2023-05-28 13:31:09\",\"28-05-2023\",\"28-05-2023 15:50\")]\n",
    "test_DF = spark.createDataFrame(test_data,schema=[\"id\",\"date\",\"datetime\",\"date_str\",\"dt_str\"])\n",
    "test_DF.show()\n",
    "test_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将这些字符串转换为date、timestamp和 unix timestamp，并指定一个自定义的date和timestamp 格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL 的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注册视图\n",
    "test_DF.createOrReplaceTempView(\"test_DF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to_date 将各种字符串转换为date\n",
    "- to_timestamp 将字符串转换为 datetime\n",
    "- unix_timestamp 将字符串转换为 整数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_doc(\"to_date\")\n",
    "# print_doc(\"to_timestamp\")\n",
    "# print_doc(\"unix_timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------------------+\n",
      "|     date2|to_timestamp(dt_str, dd-MM-yyyy HH:mm)|\n",
      "+----------+--------------------------------------+\n",
      "|2023-05-28|                   2023-05-28 15:50:00|\n",
      "+----------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select to_date(datetime) date2\n",
    "      ,to_timestamp(dt_str,'dd-MM-yyyy HH:mm')\n",
    "from test_DF\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date2: date (nullable = true)\n",
      " |-- to_timestamp(dt_str, dd-MM-yyyy HH:mm): timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select to_date(datetime) date2\n",
    "      ,to_timestamp(dt_str,'dd-MM-yyyy HH:mm')\n",
    "from test_DF\n",
    "\"\"\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pyspark.sql.functions.to_timestamp(col: 'ColumnOrName', format: Optional[str] = None) -> pyspark.sql.column.Column>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " F.to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------------------------+\n",
      "|to_date(datetime)|to_timestamp(dt_str, dd-MM-yyyy HH:mm)|\n",
      "+-----------------+--------------------------------------+\n",
      "|       2023-05-28|                   2023-05-28 15:50:00|\n",
      "+-----------------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_DF.select(F.to_date(\"datetime\"),\n",
    "               F.to_timestamp(\"dt_str\",'dd-MM-yyyy HH:mm')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**日期或时间戳转换为时间字符串**\n",
    "\n",
    "将日期或时间戳转换为时间字符串, 使用:\n",
    "- date_format函数和定制日期格式，\n",
    "- from_unixtime函数将Unix时间戳（以秒为单位）转换成字符串"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format,from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 时间计算\n",
    "\n",
    "- datediff   \n",
    "- months_between \n",
    "- last_day\n",
    "- 提取日期或时间戳值的特定字段（如年、月、小时、分钟和秒）从一个日期值中提取指定的字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+\n",
      "|name| join_date|leave_date|\n",
      "+----+----------+----------+\n",
      "|   A|2016-01-01|2023-05-31|\n",
      "|   B|2017-05-06|2023-04-09|\n",
      "+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df = spark.createDataFrame([(\"A\",\"2016-01-01\",\"2023-05-31\"),(\"B\",\"2017-05-06\",\"2023-04-09\")]\n",
    "                                ,schema=[\"name\",\"join_date\",\"leave_date\"])\n",
    "\n",
    "case_df.createOrReplaceTempView(\"case_df\")\n",
    "\n",
    "case_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**执行date和month计算**\n",
    "- 呆了多久（天）\n",
    "- 呆了多久（月）\n",
    "- 离开日期所在月最后一天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff,months_between,last_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+--------------------+\n",
      "|datediff(leave_date, join_date)|last_day(leave_date)|\n",
      "+-------------------------------+--------------------+\n",
      "|                           2707|          2023-05-31|\n",
      "|                           2164|          2023-04-30|\n",
      "+-------------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# API\n",
    "case_df.select(F.datediff(\"leave_date\",\"join_date\"),\n",
    "               F.last_day(\"leave_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**执行日期加、减计算**\n",
    "\n",
    "- date_add:date_add(start_date, num_days) - 返回 `start_date` 之后 `num_days` 天的日期\n",
    "- date_sub:date_sub(start_date, num_days) - 返回 `start_date` 之前 `num_days` 天的日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add,date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                  d|\n",
      "+-------------------+\n",
      "|2024-01-14 16:32:30|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('2024-01-14 16:32:30',)], ['d'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**提取日期或时间戳值的特定字段（如年、月、小时、分钟和秒）**\n",
    "\n",
    "\n",
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year,month,dayofmonth,hour,minute,second,dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用户自定义函数\n",
    "\n",
    "<hr>UDF函数\n",
    "\n",
    "使用UDFs涉及有三个步骤:\n",
    "- 1）首先编写一个函数并进行测试\n",
    "- 2）通过将函数名及其签名传递给Spark的udf函数来注册该函数\n",
    "- 3）在DataFrame代码或发出SQL查询时使用UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "|   id|    st|    ed|\n",
      "+-----+------+------+\n",
      "|10001|中山路|南京路|\n",
      "|10002|北京路|中山路|\n",
      "|10003|南京路|中山路|\n",
      "|10004|南京路|北京路|\n",
      "|10005|中山路|南京路|\n",
      "|10006|北京路|南京路|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[10001,'中山路','南京路'],\n",
    "        [10002,'北京路','中山路'],\n",
    "        [10003,'南京路','中山路'],\n",
    "        [10004,'南京路','北京路'],\n",
    "        [10005,'中山路','南京路'],\n",
    "        [10006,'北京路','南京路']]\n",
    "route_df = spark.createDataFrame(data,schema=\"id int,st string,ed string\")\n",
    "route_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个线路的订单数量，线路不区分起点与终点\n",
    "def get_route(st,ed):\n",
    "    return \"-\".join(sorted([st,ed]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_get_route = udf(get_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|   id|        route|\n",
      "+-----+-------------+\n",
      "|10001|中山路-南京路|\n",
      "|10002|中山路-北京路|\n",
      "|10003|中山路-南京路|\n",
      "|10004|北京路-南京路|\n",
      "|10005|中山路-南京路|\n",
      "|10006|北京路-南京路|\n",
      "+-----+-------------+\n",
      "\n",
      "+-------------+-----+\n",
      "|        route|count|\n",
      "+-------------+-----+\n",
      "|中山路-南京路|    3|\n",
      "|中山路-北京路|    1|\n",
      "|北京路-南京路|    2|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "route_df.select(\"id\",udf_get_route(\"st\",\"ed\").alias(\"route\")).show()\n",
    "route_df.select(\"id\",udf_get_route(\"st\",\"ed\").alias(\"route\")).groupBy(\"route\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_route(st, ed)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注册函数\n",
    "spark.udf.register(\"udf_get_route_sql\",get_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_df.createOrReplaceTempView(\"route_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------+\n",
      "|udf_get_route_sql(st, ed)|count(1)|\n",
      "+-------------------------+--------+\n",
      "|            中山路-南京路|       3|\n",
      "|            中山路-北京路|       1|\n",
      "|            北京路-南京路|       2|\n",
      "+-------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select udf_get_route_sql(st,ed)\n",
    "    ,count(*)\n",
    "from route_df\n",
    "group by udf_get_route_sql(st,ed)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark自带函数解决\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 与pandas DataFrame的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "            select udf_get_route_sql(st,ed)\n",
    "                ,count(*)\n",
    "            from route_df\n",
    "            group by udf_get_route_sql(st,ed)\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action操作toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>udf_get_route_sql(st, ed)</th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>中山路-南京路</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中山路-北京路</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>北京路-南京路</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  udf_get_route_sql(st, ed)  count(1)\n",
       "0                   中山路-南京路         3\n",
       "1                   中山路-北京路         1\n",
       "2                   北京路-南京路         2"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"font.sans-serif\"]=\"SimHei\"\n",
    "plt.rcParams['axes.unicode_minus']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='udf_get_route_sql(st, ed)'>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAH+CAYAAAChoGXjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+TklEQVR4nO3deXxU1f3/8fdMZrKDBAkkEIEAorJILBAs2K8LFqpQJYoLVkgrS0FcKoiKLSIVhKiIUkFxBQVkiaIguICIIq0WAqhhUUTZE0gIErJAZvv94S9Tx5AwgSRnmHk9Hw8f5s6cO/czmXMmb+49916Lx+PxCAAAwACr6QIAAEDoIogAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjLGZLsBfR44ckdPpNF3GWS0+Pl55eXmmywB80C8RaOiTNcNmsykuLu7U7eqglhrhdDrlcDhMl3HWslgskn7+PXJVfwQK+iUCDX2y7nFoBgAAGEMQAQAAxhBEAACAMQQRAABgzFkzWRUAEHqKi4vldDq9k0jrQmlpqcrKyupse2ez6Oho2WxnFiUIIgCAgHTixAlZLBadc845dbpdu93OWZp+cLvdOnbsmGJiYs4ojHBoBgAQkE6cOKGoqCjTZaASVqtV9erVU0lJyZm9Tg3VAwBAjavLQzKoPqv1zGMEQQQAABhDEAEAAMacVhA5duyYvv32WxUWFtZ0PQAAoBZ89NFHys7OrrLNp59+qg0bNtRRRT+r9jTXdevW6eWXX1Z8fLwOHDigESNGqEePHlWus3XrVr300ksqLCxUWlqa+vbte9oFAwBCl2vodbW/jV/8HPbS0lrfXl349ttvNWnSJC1evLjKdomJiRoyZIgyMzPVuHHjOqmtWntEiouL9eqrr2rChAl64oknNGzYMM2bN6/KdQoLC5WRkaEePXpo4sSJWrt27SkTGQAAqDljx47VE0884RMuCgoKdOmll2rv3r3ex9q2bat77rlHjz/+eJ3VVq0gUlpaqj//+c9q3ry5JKlFixYqLi6ucp21a9cqLi5ON954oxITE9W/f3+tXr369CsGAABe/fv318KFCyt9vvzvcLdu3byPFRQUKD093SeElLvxxhv17bffas+ePbVS769V69BMo0aN9Lvf/U7Sz7dIXrZsmVJTU6tcZ/fu3erQoYP3FKw2bdpo/vz5lbZ3OBw+F5KxWCze88g5jev0lf/u+B0ikNAvgdr3/vvv64YbbvB5bMSIEbr++uu1cePGCu0tFouuv/56rVixQsOHD/drG2cyhk/rUmi7du3ShAkTZLPZ9Mwzz1TZtqSkRElJSd7lqKgoFRQUVNp+yZIlyszM9C4nJycrIyND8fHxp1Nqndnbp4vpEk6pYu4NTOctr9uJUjAvISHBdAkIQKWlpbLb7T6PuSppW1t+vX1/fPPNN3rggQe0bds2tW/fXlOnTtWFF16obdu2acyYMdq2bZtSU1M1depUNW3aVOvWrdM999yjrKws72s0btxYGzZs0N69e3XPPfdo0qRJevjhh3Xs2DE98MADGjp0qO6//369/vrrkqT//Oc/GjVqlAYNGqSnnnrKp55t27bpvvvu83kvTz/9tFq2bKnx48fLZrNVeJ9du3bVG2+84df7Dw8PV2JiYrV/T+VOK4i0aNFCjzzyiN544w3NnDlTY8aMqbRtWFiYz6Vfw8PDq7yG/68ns5anrLy8PDmdztMpF2eZnJwc0yWgjlgsFiUkJCg3N1cej8d0OQgwZWVlxi+1Xt3tHzt2TLfccouGDBmiWbNm6cUXX9Tw4cP1zjvv6Oabb9btt9+uf/3rX5o+fboGDhyoFStWyOl0yuPxVNiW0+mU0+lUQUGBnn32Wc2ZM0fr1q3ThAkTdMstt+iRRx7RQw89pPT0dPXr109paWkKDw+v8Dr5+fk655xzfB5v1qyZd9npdFZY55xzzlFeXp5f77+srOyk39s2m82vnQinFUQsFouSk5N15513auTIkSoqKlJsbOxJ28bGxvqc5ltaWlrlNentdnulCYwvqtDA5xx6PB4PnzuCwqpVq9SgQQPdfffdkqR7771XKSkpWrlypWJiYjRq1ChJ0mOPPaaLL75YmzZtOuVrFhcXa/LkybrwwgvVqlUrPfLIIzp8+LCSkpIUFRUlm82mqKioSu/JExMTo6KiIjVs2NDv91FcXKzo6Gi/25/J+K3WZNXs7Gy98cYb3uWwsLCfX6SKS7y2bt1aO3bs8C7v2rWrWr8MAADOFjk5OTrvvPO8yw0aNND111+vAwcOeE/0kKSIiAg1adJEBw4cqPAapaWlPssNGjRQ+/btJf18VEGq3h/+pKSkak883b17t8+0itpUrSDStGlTrVq1SqtWrVJ+fr7mzZuniy++WNHR0SopKTnpoZMuXbpo+/btys7Olsvl0rJly9SpU6caewMAAASKpk2b+vzRLy4u1lVXXaVmzZr5PH78+HEdPHhQzZo1k8Vikcv1v9kvX331lc9rVnbEodypJopeccUV+uijj6rzNrRy5UpdddVV1VrndFUriDRs2FD33Xefli9frtGjR6usrMy7+2nMmDEnnX1bv359DRo0SJMmTdKwYcO0d+/eCrN3AQAIBj179tTRo0c1ffp0HThwQM8++6xcLpd69uyp4uJiPf3009q3b58eeeQRJScnKyUlRYmJiTp06JB27dqlkpISTZ06tVrbTE5O1tq1a3Xw4EF99tlnPqFGkvr27atly5bpyJEjfr3ezp07tX37dnXv3r1adZyuas8RSUlJUUpKSoXHZ8yYUek6vXv3VqdOnbRv3z61a9euWsedAAAoVxdXOrXb7ac9SbZevXqaN2+exo4dq+eee04XX3yxXnnlFcXGxmrevHl66KGHNGvWLHXt2lWvvvqqrFarWrZsqSFDhigtLU3x8fF65JFH9O9//9vvbd57770aOXKkfvvb3yoxMVFr1qzxTp2Qfj6089e//lUPPfSQXnjhhSr3oJw4cUL333+//vGPf/i8Rm2yeM6SGWL+zt41pS4uOxwqguWSyjg1i8WixMRE5eTkMFkVFRQWFqp+/fp1vt0zCSKBbNSoUbr++ut1+eWXV9pm7ty5OnjwoEaPHu3361b2Odnt9to7awYAAJxdnnzyyVPu5bjtttvq/AKDBBEAAEKAP4daqjoLtrbU/RYBAAD+P4IIACBgMXcosLnd7jN+DYIIACAgRUREVLi4FwKH2+3WsWPHzvhMWOaIAAACUkREhIqLi3X06NE6nUB5qnui4X9iYmKqvG2LPwgiAICAFRMTU6fb45TyusehGQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxtiq03j9+vWaM2eO8vPz1apVK915551KSkqqcp0pU6Zo48aN3uWOHTtq3Lhxp1ctAAAIKn4HkdzcXM2cOVNDhw5Vu3bt9Oqrr2rWrFl67LHHqlzvxx9/1FNPPaVzzz1XkhQWFnZmFQMAgKDh96GZ/fv3a8CAAerevbsaNGigXr16aefOnVWuc/jwYXk8HjVv3lwxMTGKiYlRZGTkGRcNAACCg997RDp37uyzfODAASUkJFS5zvfffy+3263hw4eruLhYnTt31pAhQxQbG3t61QIAgKBSrTki5ZxOp5YtW6Y+ffpU2S4nJ0etWrXSwIEDZbFY9Pzzz+vNN9/U0KFDK13H4XDI4XB4ly0Wi6Kiorw/I/jxOYeO8s+azxyBgj5Z9ywej8dT3ZXmzp2rr776SpMnT5bN5n+W2bp1q6ZOnapXXnml0jaLFi1SZmamdzk5OVkZGRnVLbHO7e3TxXQJQeO85RtMlwAAqCPV3iPy9ddfa+XKlZo0aVK1QogkRUdH69ixY3I4HLLb7Sdtk5aWpr59+3qXy1NpXl6enE5ndcvFWSgnJ8d0CagjFotFCQkJys3N1Wn8mwiocfTJmmOz2RQfH3/qdtV50YMHD2r69OkaMmTIKU/blaSpU6fqj3/8o9q2bStJ2rlzpxo0aFBpCJEku91e6fN0itDA5xx6PB4PnzsCCn2y7vh91kxZWZmmTJmirl27qmvXrjp+/LiOHz8uj8ejkpKSk+6taNGihebMmaMdO3YoKytLCxcuVK9evWr0DQAAgLOX33tENm/erP3792v//v36+OOPvY8/99xzmjBhgtLT05WamuqzTr9+/ZSXl6eJEyeqfv366tWrl9LS0mquegAAcFY7rcmqJuTl5fmcTRNoXEOvM11C0Ah7aanpElBHLBaLEhMTlZOTw25wBAT6ZM2x2+1+zRHhXjMAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYW3Uar1+/XnPmzFF+fr5atWqlO++8U0lJSVWus3XrVr300ksqLCxUWlqa+vbte0YFAwCA4OH3HpHc3FzNnDlTt912m1544QU1atRIs2bNqnKdwsJCZWRkqEePHpo4caLWrl2r7OzsMy4aAAAEB7/3iOzfv18DBgxQ9+7dJUm9evXS448/XuU6a9euVVxcnG688UZZLBb1799fq1evVocOHSpdx+FwyOFweJctFouioqK8PyP48TmHjvLPms8cgYI+Wff8DiKdO3f2WT5w4IASEhKqXGf37t3q0KGD9wNt06aN5s+fX+U6S5YsUWZmpnc5OTlZGRkZio+P97dUI/aaLiCIJCYmmi4BdexU3yUIHnv7dDFdwimdDd/n5y3fYLqEGlOtOSLlnE6nli1bpj59+lTZrqSkxGcOSVRUlAoKCqpc59fzSMpDTF5enpxO5+mUi7NMTk6O6RJQRywWixISEpSbmyuPx2O6HOCscTZ8T9psNr92IpxWEFmwYIEiIyN19dVXV9kuLCxMNtv/NhEeHq6ysrIq17Hb7bLb7Sd9ji+q0MDnHHo8Hg+fO1ANwTReqn367tdff62VK1fq3nvv9QkZJxMbG6vCwkLvcmlp6SnXAQAAoaNaQeTgwYOaPn26hgwZcsrTdiWpdevW2rFjh3d5165datiwYfWrBAAAQcnvIFJWVqYpU6aoa9eu6tq1q44fP67jx4/L4/GopKTkpPM3unTpou3btys7O1sul0vLli1Tp06davQNAACAs5ffx0k2b96s/fv3a//+/fr444+9jz/33HOaMGGC0tPTlZqa6rNO/fr1NWjQIE2aNEnR0dGKjIzU8OHDa656AABwVvM7iKSmpmrRokUnfW7GjBmVrte7d2916tRJ+/btU7t27RQdHV39KgEAQFCqk5mjCQkJXCcAAABUwE3vAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYIytuiscO3ZMDz30kMaPH6/GjRufsv2UKVO0ceNG73LHjh01bty46m4WAAAEoWoFkcLCQmVkZCgvL8/vdX788Uc99dRTOvfccyVJYWFh1asQAAAErWodmnn22WfVo0cPv9sfPnxYHo9HzZs3V0xMjGJiYhQZGVntIgEAQHCq1h6RYcOGqUmTJpo9e7Zf7b///nu53W4NHz5cxcXF6ty5s4YMGaLY2NhK13E4HHI4HN5li8WiqKgo788IfnzOoaP8s+YzB6onmMZMtYJIkyZNqvXiOTk5atWqlQYOHCiLxaLnn39eb775poYOHVrpOkuWLFFmZqZ3OTk5WRkZGYqPj6/WtuvaXtMFBJHExETTJaCOJSQkmC4BdYTvypoRTN+T1Z6sWh39+vVTv379vMt/+tOfNHXq1CqDSFpamvr27etdLk99eXl5cjqdtVYrAkdOTo7pElBHLBaLEhISlJubK4/HY7oc4KxxNnxP2mw2v3Yi1GoQ+bXo6GgdO3ZMDodDdrv9pG3sdnulz/FFFRr4nEOPx+PhcweqIZjGS61eR2Tq1Kn67rvvvMs7d+5UgwYNKg0aAAAgtNRIECkpKTnpYZMWLVpozpw52rFjh7KysrRw4UL16tWrJjYJAACCQI0EkTFjxvhctKxcv379lJSUpIkTJ2r27Nnq1auX0tLSamKTAAAgCFg8Z8mBpry8PJ/TegONa+h1pksIGmEvLTVdAuqIxWJRYmKicnJyguqYNyrHd2XNOBu+J+12u1+TVbnXDAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwJhqB5Fjx45p5MiROnTokF/tt27dqvvuu0+DBw/We++9V+0CAQBA8KpWECksLNSUKVOUl5fnd/uMjAz16NFDEydO1Nq1a5WdnX1ahQIAgOBTrSDy7LPPqkePHn63X7t2reLi4nTjjTcqMTFR/fv31+rVq6tdJAAACE626jQeNmyYmjRpotmzZ/vVfvfu3erQoYMsFoskqU2bNpo/f36V6zgcDjkcDu+yxWJRVFSU92cEPz7n0FH+WfOZA9UTTGOmWkGkSZMm1XrxkpISJSUleZejoqJUUFBQ5TpLlixRZmamdzk5OVkZGRmKj4+v1rbr2l7TBQSRxMRE0yUEhb19upguwS9nw9g5b/kG0yUEjbPh8z4bBNP3ZLWCSHWFhYXJZvvfJsLDw1VWVlblOmlpaerbt693uTz15eXlyel01k6hCCg5OTmmSwB80CcRaM6GPmmz2fzaiVCrQSQ2NlaFhYXe5dLSUp9gcjJ2u112u/2kz3k8nhqtD4GJzxmBhj6JQBNMfbJWryPSunVr7dixw7u8a9cuNWzYsDY3CQAAziI1EkRKSkpOetikS5cu2r59u7Kzs+VyubRs2TJ16tSpJjYJAACCQI0EkTFjxmjjxo0VHq9fv74GDRqkSZMmadiwYdq7d69uuOGGmtgkAAAIAqc1R2TRokU+yzNmzKi0be/evdWpUyft27dP7dq1U3R09OlsEgAABKFanaxaLiEhQQkJCXWxKQAAcBbhpncAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMsVWn8Z49e/T8888rNzdXV111lW6//XZZLJYq15kyZYo2btzoXe7YsaPGjRt3etUCAICg4ncQcTgcysjIUKdOnXTvvffqtdde05o1a3TllVdWud6PP/6op556Sueee64kKSws7MwqBgAAQcPvILJp0yaVlJQoPT1dERERGjBggF555ZUqg8jhw4fl8XjUvHlzvwtyOBxyOBzeZYvFoqioKO/PCH58zgg09EkEmmDqk34Hkd27d6tt27aKiIiQJLVo0UL79u2rcp3vv/9ebrdbw4cPV3FxsTp37qwhQ4YoNja20nWWLFmizMxM73JycrIyMjIUHx/vb6lG7DVdQBBJTEw0XUJQoE/WHPpkzaFf1oxg6pN+B5HS0lKfMGCxWGS1WlVUVFRpsMjJyVGrVq00cOBAWSwWPf/883rzzTc1dOjQSreTlpamvn37+mxHkvLy8uR0Ov0tF2exnJwc0yUAPuiTCDRnQ5+02Wx+7UTwO4hYrVbZ7Xafx8LDw1VWVlbpOv369VO/fv28y3/60580derUKoOI3W6vsJ1yHo/H33JxFuNzRqChTyLQBFOf9Pv03djYWBUWFvo8VlpaKpvN/xNvoqOjdezYMZ85IAAAIHT5HUTatGmjHTt2eJcPHTokh8NR5XyPqVOn6rvvvvMu79y5Uw0aNKh0jwcAAAgtfgeRiy66SCUlJfr0008lSe+88446duwoq9WqkpKSk87faNGihebMmaMdO3YoKytLCxcuVK9evWquegAAcFbz+7hKWFiYhg0bpunTp2vu3Llyu9169NFHJUljxoxRenq6UlNTfdbp16+f8vLyNHHiRNWvX1+9evVSWlpajb4BAABw9rJ4qjnjpaCgQDt37tQFF1yg+vXr11ZdFeTl5QX03BLX0OtMlxA0wl5aarqEoECfrDn0yZpDv6wZZ0OftNvtNXvWTLmGDRuqYcOGp1UUAADAL3HTOwAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhjq07jPXv26Pnnn1dubq6uuuoq3X777bJYLFWus3XrVr300ksqLCxUWlqa+vbte0YFAwCA4OH3HhGHw6GMjAwlJydr8uTJ2rdvn9asWVPlOoWFhcrIyFCPHj00ceJErV27VtnZ2WdaMwAACBJ+B5FNmzappKRE6enpSkhI0IABA7R69eoq11m7dq3i4uJ04403KjExUf379z/lOgAAIHT4fWhm9+7datu2rSIiIiRJLVq00L59+065TocOHbyHb9q0aaP58+dXuY7D4ZDD4fAuWywWRUVFyWar1lGkOmdtfYHpEoJGmN1uuoSgQJ+sOfTJmkO/rBlnQ5/09++233/dS0tLFR8f7122WCyyWq0qKipSbGzsSdcpKSlRUlKSdzkqKkoFBQVVbmfJkiXKzMz0Lvfo0UP33nuv4uLi/C3VjOnzTFcA+KJPIhDRL/Erfh+asVqtsv8qgYWHh6usrKzSdcLCwnwS0anaS1JaWppmz57t/W/o0KE+e0hwekpLS/Xggw+qtLTUdCmAF/0SgYY+Wff83iMSGxurvXv3+jxWWlpa5a6X2NhYFRYW+t1ekux2e4XAgzPn8Xj0448/yuPxmC4F8KJfItDQJ+ue33tE2rRpox07dniXDx06JIfDUelhGUlq3bq1zzq7du1Sw4YNT7NUAAAQbPwOIhdddJFKSkr06aefSpLeeecddezYUVarVSUlJXI6nRXW6dKli7Zv367s7Gy5XC4tW7ZMnTp1qrnqAQDAWc3vQzNhYWEaNmyYpk+frrlz58rtduvRRx+VJI0ZM0bp6elKTU31Wad+/foaNGiQJk2apOjoaEVGRmr48OE1+gbgH7vdrv79+3PYCwGFfolAQ5+sexZPNQ+EFRQUaOfOnbrgggtUv359v9bJzc3Vvn371K5dO0VHR59WoQAAIPhUO4gAAADUFG56BwAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAPD/rVixgvub1TGCSIhhkCHQ0CdhyoIFC3Tw4EHvcnFxsd59911t3bpVW7Zs8f5/8+bN5ooMAX5fWRVnnwULFujKK69UkyZNJP1vkDVr1kw2m00Wi0Uej0cOh0MpKSlmi0VIoE8ikKxbt05r165Vx44dddttt2n16tXq3bu3XnjhBTVv3lwej0fffvutLrzwQvpjLSKIBDEGGQINfRKBJDw8XBkZGVq1apXGjRuniIgITZw4UR9//LHGjh0rSRo9erT3Z9QODs0EsfDwcD377LNq2bKlxo0bp3//+9/q27evrFarxo4dq4cffliNGjVikKHO0CcRKHbs2CGLxSKbzaZLLrlENptNLpdLbrfbp53FYjFUYeggiAQpBhkCDX0SgSI3N1fTpk3ToUOHtHz5cj3xxBMaPny4evbsqaVLl0qSvvvuO23fvl0nTpzQd999Z7ji4MahmSBUPsiKioq0fPlyrV69WsOHD9eOHTt8Bpnb7fYOsrZt2xquGsGMPolAkpCQoJkzZyorK0tvvfWWGjdurMaNGys5OVmjRo1SixYttGTJElmtViUlJWnx4sV64IEHuCNvLeGmd0GsfJCdc845Gj58uGJiYjRq1CglJSXJ4/HIarXK7XbL6XQyyFAn6JMIFE6nU2VlZbJYLFqxYoV++9vfqmnTpsrMzFTPnj0VFxenuXPn6uabb1Z4eLjpcoMaQSRIMcgQaOiTCCRffPGFZs2apZiYGHk8HkVHR+vJJ5/Uu+++q9jYWPXs2VNvvPGGLrroInXp0sV0uUGNQzNBasOGDT6D7IsvvtCTTz4pu92ujRs3qmfPnvJ4PPr6668ZZKgT9EkEmmuvvVY33XSTJOkvf/mLnnvuOe3fv182m03btm1TXl6eiouL6Y+1jCASxBhkCDT0SQSSX06Mjo6O1mWXXebzuNvt1ksvvWSktlBCEAliDDIEGvokAsnSpUu1cuVKeTwe1a9fXykpKRo/frzKysq8bQoLC7Vt2zZddNFFBisNbgSRIMYgQ6ChTyJQpKamqnPnzgoLC5Pb7faeRj5y5EifwPzBBx/owIED9MdaxGTVIOV2u+VyuXwGWXh4uA4dOlRhkDVt2lQ9e/Y0WC1CAX0SwMkQRAAAIcftduudd97RDTfccMq2JSUlio6OroOqQhNXVg1Sbrdbb7/9tl9tS0pKarkagD6JwGK1WvXuu++est3s2bM1efLkOqgodBFEghSDDIGGPolAY7P9PE3y7bff1qJFi5SZmanFixd7A/OaNWv0ySef6C9/+YvJMoMek1WD2C8HmdPplNVqlcfjUVhYmG644QbvIBs/frzhShEq6JMIJOVzk5YvX65LL73U+/iXX36pSy65RK+88oruu+8+tWrVylSJIYEgEsQYZAg09EkEqqFDh3p//uKLLxQXF6dhw4bpN7/5jcGqQgNBJEQwyBBo6JMw6ciRI6rqXA2n06mLLrpI+fn5ioyMVGxsbB1WF1oIIkGKQYZAQ59EoHj33XeVmZlZ6U0Vi4qKNHr0aG9/PXHihO644w717t27LssMGQSRIMQgQ6ChTyKQxMXFacKECXr88cdP+nxsbKxeeeUV7/LHH3+sdevW0R9rCUEkCDHIEGjokwgk//d//1fl8xaLRUVFRTp8+LBatGih5s2bKyoqqo6qCz0EkSDEIEOgoU8i0M2aNctned26dVq8eLEeffRRnX/++Tr//PMNVRb8CCIhgkGGQEOfhGnlhwL79u0rl8slq9Uqt9utPn36qHfv3nK5XHrsscf0z3/+U02aNDFcbfAiiAQxBhkCDX0SgcLtdntvtpiWlnbSNtdee60KCgo0adIkTZs2TWFhYXVZYsggiAQpBhkCDX0SgcTj8ejKK688ZbvbbrtNv/nNb+iLtYib3gUpl8ulOXPm6I477qiyndvt1vbt29WuXbs6qgyhij4J4GQIIgCAkPTyyy9ryJAhKioqUnh4uPcWBL9mtXJbttrEoZkgxiBDoPriiy98LvFebvv27XrppZc0ePBg9oig1n311VeSpPHjx2vfvn2VtmvcuLH+9a9/1VVZIYcgEsQYZAhEhYWFmjVrltxut7p37+7zXFxcnLp27arnn3+ePolaFxER4f15zpw5Ps+5XC6tX79el156qT766CO53W7+0VZLCCJBjEGGQFS/fn2NGDFC8+fP16WXXurT75o0aaJbb71VK1euNFghQlFkZKROnDghi8WiMWPG6JlnntGiRYt0xRVX6He/+x3fj7WI32yIiIyMlMVikdVq1YMPPqjo6GgtWrRIkZGRDDLUudTUVDVt2lRr166t8Jzb7fbepReoK9nZ2Xr44Ye1detW2Ww2WSwWbz+Mi4szXF1wY49IiMjOztZrr72mgQMHMshgxFtvveUzT6l+/frKzMzUTz/95NNu9+7dqlevXh1Xh1BSUFCgn376yXtdG4vForZt2+qaa67R7NmzZbFYVFBQII/HoyNHjsjhcKhhw4aVzrPDmeG3GoQYZAhEhw8fVnh4uHc5MjJSnTt31pEjR3zanXvuuerbt29dl4cQ8s033+i1115TaWmpxowZo/z8fIWHh+vqq6/WVVddpQEDBuiee+6Rw+HQXXfdJZfLpccff1ytWrUyXXpQ4vTdIPTpp596B1nz5s2Vl5en2bNnS/p5t/eAAQNkt9vlcDhks9kYZABC0qhRo3THHXdoxowZev755/Xee++pcePGWrRokZ566imNHDlSM2bMMF1m0OOfwEHo8ssv1+WXX65Ro0YpPT3dO5DKB9l5553HIEOdW7Vqlex2+ynbeTweORwO/f73v6+DqhDKrFarOnTooOjoaH3++efasGGDbrvtNp+9yah9BJEgxiBDIPnqq698LpO9fv16de3a1afN5s2b1bZtW9ntdoIIat0vDwhcdtlluuyyy1RUVKSGDRuquLhYLpfLYHWhgyASxBhkCCSjR4/2WR48eLD+9re/+Tw2bdo0devWrcL1RYDa4HQ6vT+XlZXJ4/HIbrfr/vvvl8Ph0NChQ3XixAlJvpdDQM0iiAQxBhkC2cn2yLVv316bN28miKBOlJSUeH8eOHBglW0XLlxY2+WELIJIEGOQIVA5nU5vCP6lCy+8UG+++aaOHz+uyMhIA5UhlFx77bVyu9168MEHZbVaFRER4XP40O1268SJEyftq6g5nDUTxJYsWaLrr79e+fn5pxxkTZs2NVgpQlFRUZFiY2N9HnO73Vq6dKn69Onj18RWoCYVFxcrJibGdBkhhyASQhhkCDT0SQSSv/zlL3ryySfVqFEj06WEFK7rHULuuusu5efnmy4D8KJPAiCIAAAAYwgiAADAGIIIAAAwhiACAACM4ToiAABIevrpp1WvXj3TZYQcTt8NIUeOHFG9evVks5E/ERjokzDJ6XRW6Hv/+Mc/9PDDDys6OtpQVaGH0R/Efj3I4uLiGGQwij6JQDJ27FilpqaqT58+io6O1p49e+TxeDRv3jyVlZXJYrF47wb96/sioeYQRIIYgwyBhj6JQOJ0OnX8+HE98MAD6t+/vzZu3Kibb75Zs2bN0u233y5JeuONN055iwycGSarBrFfDrI1a9YoMzNTN998szZt2qRLLrlEKSkpys7OVmpqqulSESLokwgkYWFhGjhwoCZOnKj3339fhw8fVqdOnWSxWNS9e3d1795d0dHR3ISxlrFHJIiVD7I//vGPmjx5smw2m88gk6S33nqLQYY6Q59EICgtLdW8efPkcDgkSZ9//rkiIiLk8Xi0Z88enztDn+wu0ahZBJEgxCBDoKFPIpCUlZUpNjZWZWVleuCBB5SYmKhx48bphx9+0IIFC+RyuTRr1ix5PB4VFBRo1qxZGjx4MJOqawlnzQSho0eP6v3339enn36qevXqKTExUXfddZd++OEHvfvuu/rxxx+VkpIij8ej//73v+rWrRuDDLWKPolA5HK5tHLlSi1btkxjx45VUlKS7r//fl1xxRWKj4+Xy+VSWFiYHA6HunfvLquV2Qy1gSASxBhkCDT0SQSKnJwcbdq0SWFhYTp48KCaN2+uK664QqtXr1bbtm2VlJSk8ePH6+677+ZuvLWMUR6kcnJy9OGHH8pisahbt276/vvvJUnXXnutUlJS1K1bN3344Ydq3bq1LrvsMr7wUevokwgku3fv1vr161VYWKioqCi99957crvdKioq0nfffSe3263U1FRlZWWZLjXoMdKDFIMMgYY+iUDTvn173XTTTbrpppu0f/9+jRgxQu+9954WLFigESNG6J133tGGDRtMlxn0OAAbxNq3b6/+/ftLkt5++22NGDFC5UfiFi5cKLfbrZYtW6p3794my0QIoU8ikOTm5uqrr76SJDVr1kxPPfWUdu/e7d0b5/F49M9//lMlJSVccK8WEUSCGIMMgYY+iUDRsGFDOZ1OffbZZ/J4POrUqZMkaf78+SorK/O2s9ls2rBhg/7v//7PVKlBjyASpBhkCDT0SQSStm3bqm3bthUeHzt2rM/y5s2bFRMTU1dlhSTOmglx5YPs/PPPN10KIIk+CYQagggAIOS4XC7df//9mjZtWpXt8vPztXv3bnXu3LmOKgs9HJoJciUlJfr6669PemEoj8cjp9Mpp9Op3/3udwaqQyiiTyIQhIWF6aeffqqyzfHjxzVlyhTFx8cTRGoRQSTIHT16VAsXLpTNZjvppbP37t2r5s2b86WPOkOfRKAICwuTJI0ePVrHjx/33v05KipKTz31lGbOnKmIiAjdd999hisNbgSRIJeYmFjlrsfBgwcrIyOjDitCqKNPItD89NNPeuihhyT9vFfuiSee0KZNm/T9999rypQpCg8PN1xhcCOIhDhuMIZAQ5+ECb+eHH3JJZdoypQpql+/vqGKQgdBBAAQUlwulz788ENVdq7GiRMnlJmZ6V2OjIzU5Zdfrnr16tVViSGFIBKkPvroI7/uXFp+W3agttEnESimTZumQ4cOVfq8x+NRYWGhd3nDhg3KycnR0KFD66K8kEMQCUJut1tffvllpZMBf8npdNZRVQhl9EkEkj//+c8699xzKw0WkZGRuuOOO7zLX3zxhZYvX15X5YUcgkgQslqtGjdunF9tBw8eXMvVAPRJBJZGjRr5LP86HHs8HmVlZWn37t264YYbdOGFF3qvBIyaRxAJcUwMRKChT6KueTweDRgwwLscExOjhg0b6sUXX5Tdbtcf//hHg9UFP4IIACAkuVwuSdLkyZNltVpltVrldrvldrvVuHFjjR8/XuPGjVNUVJSuvvpqw9UGL4JIkMvPz9dHH30ku91+0n9pnjhxwkBVCGX0SQQCp9Ppvdli48aNT9qmadOmGjVqlF599VVdeeWV3gugoWYRRIKcw+HQwYMHFRYWVuFL3+VyKSUlxUxhCFn0SQSCsLAwTZgwoco2x48fV1RUlDIyMgghtYib3gEAQlZJSYmmT5+uu+66S7GxsT7PffHFF3rmmWd07bXXatCgQYYqDH5W0wWg9u3bt0+5ubk6ePCgDh48qJycHOXl5ZkuCyGMPolAERkZqZKSEr377rsVnrv00kv19NNP6/PPPzdQWejg0EwIGD16dIXH2rVrp/HjxxuoBqBPInBYrVbde++9euihh9SnTx81aNDA5/mmTZvK7XabKS5EEERCQJMmTTR9+nTv8siRI/nCh1H0SQSSc889V3/4wx+0dOnSCodgfvrpJ04pr2UEkRDw60HEoIJp9EmYdvfdd/vccsDpdKqgoECbNm3yaVdQUKAOHTrUdXkhhSACAAg5d999t8LDw0/Zzm63KzExsQ4qCl0EkSA1d+5chYWFKSwsTEVFRT53kiwuLvYuu91ulZWV6fbbbzdVKkIEfRKBpG3btqZLwP9HEAlSubm5slqtstlscjqdysnJ8T7ncrm8y263W8ePHzdVJkIIfRKB5O9//7tfe0QkqaysTJMmTarlikIX1xEJAffcc0+FiYEzZswwWBFCHX0SpmVlZflcpOyZZ57R3/72N582r732mnr16qX4+HilpqbWcYWhgz0iIYCJgQg09EmY1rlzZ59lm81W4aq+KSkp8ng8hJBaxgXNAAAhzePxeG+A90vt27dXVlaWgYpCC3tEQkB+fr4efPBB73JBQYFmzZqlv/71rwarQiijTyKQuFwutW/fvsLjbdu21bPPPqvCwkLVr1/fQGWhgTkiIeDzzz+X3W6X1WqVx+ORx+NRdHS0OnbsaLo0hCj6JM4W+fn5atSokekyghpBBAAAGMMcEQAAYAxBBAAAGEMQAQAAxhBEAKAa9uzZY7qEGpWTkyOn02m6DIQwgggA+Gnz5s167LHHVFRUZLqUGvPOO+/ohRdeMF0GQhhBBKjEyJEjtWbNmgqPL1q0SEOHDtWAAQO4/0QQWLNmjUaOHHnKdgUFBZoxY4buv/9+xcbG1kFltWfGjBneS+rfcccd2r9/vz766CPDVSFUEUSAati8ebPeffddDR06VA8//LC6dOlS5zVs2bLlpAEp0KxZs0ZbtmwxXUaNeeONN9SzZ09dcMEF1V530aJFOnToUC1UdeYiIiI0cuRILV68WIWFhabLQQgiiADV8MMPPyg5OVmpqanq2LGjevfuXec1EETq3uHDh/XVV1/phhtuOK31MzMzlZeXV8NV1ZykpCR17dpVq1evNl0KQhBBBKgGp9Mpm407I4SaL7/8Ul26dPH7tvFno8suu0xffPGF6TIQgvhGRdA7dOiQ7rrrLj333HNq3Lix9/Gbb75Z48ePV/v27eVyuTRv3jx98sknslqtuv76631e49FHH9XWrVt91pWkO++8U1dccYXftSxYsECrVq1SWFiYrr76am3fvl0FBQWaNm2aJCk7O1vz5s3Tnj171LhxY91444267LLLarSGRx99VO3atVP37t01d+5cbd++Xa+++qqs1p//XXL8+HG9/vrr+s9//iO3263U1FSlp6d750WMHDlSN910k882y1/z5ptv9tYlSVu3blVmZqYk+fz+CwsL9frrr3tvxZ6amqpBgwYpMjLS7/fx5ZdfavHixcrNzVX9+vX1hz/8Qdddd533+UOHDumll17S9u3b1ahRI11++eWaP3++Fi1a5Pc2yu3Zs+ekh2Ty8vL02muvadu2bZJ+vknakCFD1KBBA23ZskUTJkzwti3/OT4+3js/o6Z88MEHWr58uX766Se1bNlSd9xxh5KTk73Pr1mzRosWLVJhYaEuvfRSOZ1O2e12n9do27at9u7dK4/Hw92QUacIIoCkZcuW6YMPPtDtt9+uxMREzZs3T4cPH/Y+P2zYMJWWlmrVqlX64YcfNGzYMEnyCTansnbtWn344YcaMWKESktL9cILL+hPf/qT2rZtK0k6cOCAJk2apJ49e2rgwIHKzs7W9OnTdc4556hjx441UkO5Q4cO6dFHH1W3bt106623+vzhefLJJ7Vv3z4NHjxYNptNc+fO1eOPP66JEyd6w0pVJk+eLEl68cUX1apVK1199dWSpIYNG3rbTJ06VcXFxbr77rvlcDj06quv6sSJE7r77rv9qj83N1fTpk3T1VdfrcGDB2vXrl2aPXu2mjdvrpSUFLlcLk2aNEkREREaNWqUDh06pLlz51bnV+TjyJEjiouLq/D4jBkzVFhYqHvvvVcOh0MLFizQ7Nmz9be//U2tWrXy/i7Gjh2roUOHqlWrVhUCwJlavXq15syZo0GDBqlFixZasWKF/vnPf2r69OmqV6+evvvuO82cOVO9evVSt27dtHLlSq1fv94bcMvZbDZFRkbq2LFj3OANdYogAkhavny5rrrqKl177bWSfv6jOWbMGO/zTZs2lSRlZWUpKipKrVu3rvY2vvvuO1188cVKTU2VJH344YdyOBzeILJkyRKdd955GjJkiCSpXbt2ysrK0qeffqqOHTvWSA3lPvvsM40aNUqXXnqpz+NbtmzRN998o3/84x+6+OKLJUmNGjXSww8/rKysLHXt2vWUr11eV1RUlOLi4irUuWXLFm3btk1PPPGEWrZsKennM1Jef/11DR8+3K8/1IcPH5bb7dbll1+u888/XxdddJGaNWumJk2aSPp5UnFOTo4yMjK8ewby8vK0dOnSU772yVgsFrnd7gqP5+XlKSUlRSkpKZJ+7idHjx71vv9fvvemTZue0WdWmbfeeku9e/fWNddcI0lq0aKFBg8erA0bNujKK6/UihUrlJSU5O1XF154YaVnCbndbr/CJlCTCCIISb/8o1JSUqKjR4/q/PPP9z7WokULRUdH1+g2mzVrpk2bNqmgoEClpaXav3+/0tLSvM/v2bNHu3bt8jm0IanG/wUtSSkpKRVCiCTt3LlTVqvV55bobdq0UVRUlHbu3FlpEKnOvTPLLwj2wAMPVHguLy/PG7iqcsEFFyg5OVmPP/64OnXqpDZt2ig1NdW7dyg3N1dhYWHeoCP9/Af4dINIXFycjhw5UuHx3r17a968eTpw4IDOP/98dezYsU7vIFxaWqq8vDytWLFCK1as8HkuJyfH+/82bdp4H7fb7T7L5crKylRWVnbWn5qMsw9BBCHpl4ddyv+I/vpfgjX9L8OWLVvq6NGjGj58uCTpqquuqvCHvWvXrrrpppt8HquNCZIn+0MkVR4oLBZLlWEjPz+/Wtu3Wq2aPHlyhbkI/t5u3WazadKkScrOztb27du1du1azZ8/X3//+9/Vvn177+v+cr7DyfZo+KtVq1baunWrfv/73/s8ft111+mSSy5Rdna2tm3bpmXLlumKK67QX//619Pe1um49dZb9Zvf/MbnsfLDKyfby3Gyvr1161a1atWq9ooEKsE+OAS9sLAwST//i6/cf/7zH+/PMTExqlevnnbu3Ol9bP/+/TV+9cyXX35Z9913n2bMmKGXX37ZG0jKnXfeecrPz1fLli29/23fvl3r1q3zaRceHu7zXmpS69at5Xa7fSbF/vDDDyopKfEeVrBarTpx4oT3+e+///6kp6ba7faT1nneeed5/ziWv0+LxaKlS5equLjYrzq3bNmiFStWKCUlRbfeequmTJmixo0b65NPPpEkJSYmyuVy6ccff/Sus337dv9+CSeRmpqqzZs3q6SkxPvYiRMnNHv2bMXExOiaa67RqFGjdPPNN+uTTz6Ry+XyWb+y38WZioqKUqNGjXTs2DGffvPxxx97329CQoJ++OEH7zoul8unr5dbu3atfvvb39Z4jcCpEEQQ9OLi4lSvXj19/PHHcrvd2rhxo1auXOnTpnfv3lq1apU+/PBDbd68WdOnT6/xMwciIiK0atUq5eTkKD8/X7m5uT7/Sk9LS9OePXs0a9YsbdmyRStXrtQbb7yhqKgon9dp06aNfvzxR3355Zf6+uuvK+ySPxMdOnRQx44dNXPmTK1bt05ffvmlpk2bpjZt2ngv3taiRQt9/vnnOn78uA4cOKCZM2eqXr16FV7r/PPP1/r165Wdna2srCx9/vnn3m1cdNFFmj59uv773/9q8+bNmjlzpvbu3asGDRr4VWdYWJgWLFig5cuX69tvv9WaNWuUl5enhIQESVKnTp3UrFkzvfDCC9q4caM++OCDCp95dZxzzjnq3r273nzzTe9jERER2rRpk1599VV98803+vrrr7V+/Xo1atTIG37LtWnTxhsOPv/8c59QtHPnTu+8ktPRv39/ffTRR1q6dKm2bdumOXPmaNWqVYqPj5ck/eEPf/BO5s3OztaMGTNUUFDg8xo7duzQli1bdOWVV552HcDpsniqc3AXOEtt3rxZr7zyio4ePark5GQNHTpUo0aN8p6+63Q6tXDhQn322WcqKyvTNddco08//bTCaaqLFi3S1q1b9eijj1a7hvfff19vvvmmwsLCVFJSIo/Ho3r16mnMmDG68MILJf3v9N3du3crLi5OvXr1qnAqsSS9/fbbev/991VUVKQOHTro73//u991/PJU25P59em7Xbt21Z///Gfv3IHc3Fw999xz2rNnj84991wNGDBAK1asqPCax48f14svvqisrCy53W5dd9113sNOhYWFmjNnjrKysmSxWHTxxRcrPT3d58yaU/nss8+0dOlS5ebmKioqSt26dVN6erp3Ts2BAwc0a9Ys7dixQ02bNtUVV1yh119/vcLpu2vWrNHixYtPeUptUVGRxowZo/T0dO/8mgMHDuj111/Xjh07VFZWpjZt2ig9Pd1nbkp5uxdeeEE7d+5UVFSURo4cqUsuuUTSz6dhp6enq0+fPn6/918rP323oKBASUlJuuWWW3wO1Xz22Wd66623dPjwYV1yySWyWCzeK6oWFhZq3LhxuuWWW9S9e/fTrgE4XQQR4Ax5PJ4q5x9YLBYdPHhQo0eP1ogRI9SkSRNZLBbvmSLl1+k4U263u8p5HFar9ay4PkRtvY/y63qcznVEyv3www/KyMjQ1KlTa2xS59y5c9WlSxdvGK1r06dPV3x8vAYMGGBk+wCTVYEztHXrVp8LV/1aRESEZs+erWuvvVaLFy/WkSNH5HK5FBcXp4svvlj9+vWrkTruvvvuKi8jfrLTdQNRIL+PVq1a1WgIkaSDBw8aCyHSzze9i4mJMbZ9gD0iwBk6fvy4cnNzK33earWqefPmtV7Hvn375HQ6K32+cePGNX5Kcm0IlvcBwD8EEQAAYAxnzQAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAw5v8BXKebj9ymWF0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(kind=\"bar\"\n",
    "        ,x=\"udf_get_route_sql(st, ed)\"\n",
    "       ,y=\"count(1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.createDataFrame(pd_df).show() # pandas dataframe 转spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "378.971px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
